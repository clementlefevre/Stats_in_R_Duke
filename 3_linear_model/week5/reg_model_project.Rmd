---
title: "Modeling and prediction for movies"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

#### *Clement Lefevre*
#### *13th April 2017.*

### Load packages

```{r load-packages, message = FALSE}

library(dplyr)
library(tidyr)
library(statsr)
library(GGally)
library(tm)
library(SnowballC)
library(gridExtra)
library(grid)
library(stringr)
library(ggplot2)

source('multiplot.r')

Sys.setlocale("LC_ALL", "C")
```

### Load data



```{r load-data}
load("movies.Rdata")
```



* * *

## Part 1: Data

This dataset, as described in the instructions does consist in a **random sample** of **651 US movies** extracted from both **IMDB** and **Rotten Tomatoes** databases.

All have been released between 1970 and 2014. 

Assuming this is an observational study with random sampling, it should be *representative* of US movies. By using random sampling, the resulting prediction model should be *generalizable* to US movies released between 1970 and 2014.


#### Scope of inference
As we are in a context of **observational studies**, we might conclude some association patterns, but contrary to an experimental study with random samples,  **no causality** that might be deduced from it.


* * *

## Part 2: Research question

As i have alread worked on this dataset for the Bayesian Inference project to predict the `audience_score`, this time i'd like to focus on the relationship between the **textual elements** (title and crew names) of a movie and its audience score. 

**Does the title and  crew are related to the `audience score` ?**


* * *

## Part 3: Exploratory data analysis


First, a quick glance at the dataset :

```{r}
dim(movies)
summary(movies$audience_score)
ggplot(movies,aes(x=audience_score))+ geom_histogram(bins = 25) + geom_vline(xintercept = mean(movies$audience_score),color='blue')

```

With a small dataset size (651 samples), `audience_score` our target for the prediction is left-skewed, with a mean at 62%.


### New variables

Some data preparation is necessary in order to get three new variables :

- `title length` (continuous)
- `title_score` (continuous) : the average audience score based on the content of the title.
- `crew_score` (continuous) : the average audience score based on the names and surnames of the crew.



#### Title length

`title_length` is quite trivial to create :

```{r}
movies <- movies %>% mutate(title_length = nchar(title))
summary(movies$title_length)
```

Just for my curiosity, let's display the shortest and longest titles :

```{r}
movies[which.min(movies$title_length),]$title
movies[which.max(movies$title_length),]$title

```

Well, the min is *Max*, and the longest is also the *most dangerous* !

How does the `title_length` evolve over time ?


```{r}
groupy_year<- movies %>% group_by(thtr_rel_year) %>% summarise(title_length = median(title_length), number_movies=n())
ggplot(groupy_year,aes(x= thtr_rel_year, y= title_length)) + geom_line()
```



With the exception of an outlier in 1972, the `title_length` does not follow a trend over the years.


Let's look at the relationship with `audience_score` :
```{r}
p<-ggplot(movies,aes(x=title_length,y=audience_score,color=title_type)) + geom_jitter(alpha=.6)+geom_hline(yintercept = mean(movies$audience_score),colour="blue", linetype="dashed")
p+ annotate("text", x = 60, y =mean(movies$audience_score)*1.05 , label = "Average audience score", color="blue")

```

Unless the title is longer than 45 characters, there does not seem to make any difference for the audience score.
What is interesting though is the fact that Documentary movies are all above the mean score.

We can quickly see whether the title length is somehow related to the type of movies :

```{r}
groupy_year<- movies %>% group_by(title_type) %>% summarise(title_length = median(title_length), number_movies=n())
p0<-ggplot(movies,aes(x= title_type, y= title_length, fill=title_type)) + geom_boxplot()+ theme(legend.position="none")
p1<-ggplot(movies,aes(x= title_type, y= audience_score, fill=title_type)) + geom_boxplot()+theme(legend.position="none")

multiplot(p0,p1,cols = 2)

```

Indeed, short title correspond to TV movies, and longer to Documentaries. To be noted here the better audience_score for Documentaries.


#### Word Count matrix : title_score and crew_score

In order to create the new categorical variables `title_score` and `crew_score` i use the [**Text Miner** package](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) to compute the frequency of each words in all titles and crew names. 

I will get a matrix with all movies title as row and all used words as column, with the frequency of use as value.

All this process is well explained in the ["Analytical Edge MIT15071" online course](https://www.edx.org/course/analytics-edge-mitx-15-071x-3), that i warmly recommend as a complement to the current course.

```{r}

create_DTM <- function(df, contento){
  movies$id<-rownames(movies)

# 1-Create corpus
myReader <- readTabular(mapping=list(content=contento, id="title"))
tm <- VCorpus(DataframeSource(movies), readerControl=list(reader=myReader))

# 2- Clean the word : lower case, stop word, etc..
tm = tm_map(tm, content_transformer(tolower))
tm = tm_map(tm, removePunctuation)
tm = tm_map(tm, removeWords, stopwords("english"))
#tm = tm_map(tm, stemDocument)

# 3- create a Matrix of frequency of use
dtm<-DocumentTermMatrix(tm)
return (dtm)
}

dtm<- create_DTM(movies, "title")

```

We convert the DocumentTermMatrix to a dataframe, and replace the word count per title by the presence of not (0/1):

```{r}

convert_to_DF<- function(DTM){
  dtm_df<-as.data.frame(as.matrix(DTM))
  colnames(dtm_df)<- make.names(colnames(DTM))
  
  max(dtm_df)
  m <- as.matrix(dtm_df)
  m[m>1] <- 1
  dtm_df <- as.data.frame(m)
  return(dtm_df)
}

dtm_df<- convert_to_DF(dtm)
```

We can now compute the average audience score per word :
```{r}

compute_average_score_per_term<- function(dtm_df, min_appearance){
  dtm_df$audience_score<-movies$audience_score
  dtm_df_score<- dtm_df %>% mutate_each(funs(.*audience_score))
  total_score<- dtm_df_score %>% summarise_each(funs(sum)) %>% gather(term,score_total) %>% arrange(desc(score_total))
  total_count <- dtm_df %>% summarise_each(funs(sum(.>0))) %>% gather(term,movies_count)
  result_df<- merge(total_score,total_count,by = "term")
  
  #We compute the average audience_score, and remove the words that appear in less than 5 movies :
  result_df <- result_df %>% mutate(average_score = score_total/movies_count) %>% arrange((desc(average_score))) %>% filter(movies_count>min_appearance,term!="audience_score") 
  
  return(result_df)
}

result_df <- compute_average_score_per_term(dtm_df,min_appearance = 2)
```

Let's see which words of the titles score the best on average  :
```{r}
head(result_df,10)
top_words<- head(result_df,10)$term
```


And the worst average audience_score :
```{r}
tail(result_df,10) %>% arrange(average_score)
worst_words<- tail(result_df,10)$term
```

Now we do the same for the actors and directors :

First, i combine all names and surnames into a single column :

```{r}
movies<- movies%>% mutate(crew= paste(actor1,actor2,actor3,actor4,actor5,director))
```

And now compute the average score per name :

```{r}
dtm.crew<- create_DTM(movies, "crew")
dtm_df.crew<- convert_to_DF(dtm.crew)
result_df.crew <- compute_average_score_per_term(dtm_df.crew, min_appearance = 3)

top_crew<- head(result_df.crew,100)$term
worst_crew<- tail(result_df.crew,100)$term


```


And how does it look like ? 

```{r}
head(result_df.crew,10)
tail(result_df.crew,10) %>% arrange(average_score)

```


```{r}
# count number of occurence of top title word :  and worst title words :
movies<- movies %>% mutate(good_title= str_count(tolower(title),paste(top_words, collapse='|')))
movies<- movies %>% mutate(bad_title= str_count(tolower(title),paste(worst_words, collapse='|')))

# sum the title score 
movies <- movies %>% mutate(title_score=good_title-bad_title) 


# count number of occurence of top crew name :  and worst crew names :
movies<- movies %>% mutate(good_crew= str_count(tolower(crew),paste(top_crew, collapse='|')))
movies<- movies %>% mutate(bad_crew= str_count(tolower(crew),paste(worst_crew, collapse='|')))

#sum the crew score
movies <- movies %>% mutate(crew_score=good_crew-bad_crew) 

# convert to factor for better plotting
movies$title_score_factor<- factor(movies$title_score)
movies$crew_score_factor<- factor(movies$crew_score)


# !!!! i remove the zero score to improve lisibility : 
p0<-ggplot(movies %>% filter(title_score!=0),aes(x=audience_score,y=imdb_rating,col=title_score_factor)) + geom_jitter(alpha=.6)
p1<-ggplot(movies %>% filter(crew_score!=0),aes(x=audience_score,y=imdb_rating,col=crew_score_factor)) + geom_jitter(alpha=.6)


multiplot(p0,p1)

```

Well, it seems the title and crew score are in line with both imbd_rating and audience_score.


Nota Bene : The way i proceed to compute the "text score" is not really "kosher". I should have created a new column for each of the top and worst title word/crew name, or even better, one column for each word ! But the aim what to give a try, so we keep only two new variables :

- **title_score** (numerical)
- **crew_score** (numerical)


How about correlation ?

```{r}
ggpairs(movies, c("title_length","audience_score","title_score","crew_score"), title = "")
```


As we see here, the two new variables title_score and crew_score are not correlated together.
Their Pearson correlation with audience_score is respectively : **$\rho =0.22$** and **$\rho=0.504$**.
The title_length can be ignored with **$\rho$=0.06**


Finally, we remove the intermediate variable we created to compute `title_score` and `crew_score`
```{r}
movies <- movies %>% select(-one_of(c("good_title","bad_title","good_crew","bad_crew","crew")))
```



* * *

## Part 4: Modeling

Now that we have a better overview of the variables, we can proceed to the model building :

First, we can remove predictors that seem irrelevant :

- All actors and director columns, already included in crew_score,
- all url variables,
- `title` : already in a way in title_score,
- `audience_rating` : a subversion of audience_score,
- all critics variables, much too correlated to the other scoring variable `imdb_rating`.
- `imdb_rating` ,  $\rho$ = 0.864 with `audience_score` : **my aim is not to find the best model, but to evaluate the predictive power of the text content. Thus i exclude it.**


We get then a new set of predictors :

```{r}
actors<-grep('actor',colnames(movies),value = TRUE)
url<-grep('url',colnames(movies),value = TRUE)
critic<-grep('critic',colnames(movies),value = TRUE)
audience<-grep('audience_rating',colnames(movies),value = TRUE)
exclude_cols <-c(c("title","director","audience_rating","imdb_rating"),actors,critic,url)

movies_simple<- movies %>% select(-one_of(exclude_cols)) %>% na.omit()

```


Now we have a lighter dataset, we can build a model and use the step function to select the best one  based on the Aikike information criteria (AIC):

```{r}


lm1<-lm(audience_score~. ,data=movies_simple)

best_lm<-step(lm1,direction ="backward",trace = 0)
summary(best_lm)
```
Some variable do not seem to have a conclusive significance level, let's drop them :

```{r}
movies_simple <-movies_simple %>% select(-one_of(c("mpaa_rating","thtr_rel_year","dvd_rel_year")))
lm2<-lm(audience_score~.,data=movies_simple)

best_lm<-step(lm2,direction ="backward",trace = 0)
summary(best_lm)

```

Finally, both  crew_score  and title_score did pass the model selection with a p_value  below a reasonable significance level. Again, i did choose not using predictors too close to the target, i.e. of the rating/scoring family, hence the low adjusted determination coefficient $R^2 = 0.444$. Thus, i do no expect good results on the prediction.

Let's plot some chart to check the validity of the selected model :

```{r}
p0<-ggplot(data = best_lm, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals") + ggtitle("Residuals vs Fitted values")

p1<-ggplot(data = best_lm, aes(x = crew_score, y = .resid)) +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("crew_score") +
  ylab("Residuals") + ggtitle("Residuals vs crew_score values")

p2<-ggplot(data = best_lm, aes(x = imdb_num_votes, y = .resid)) +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("imdb_num_votes") +
  ylab("Residuals") + ggtitle("Residuals vs imdb_num_votes values")

p0
p1
p2
  
```

The residuals vs the fitted values are **slighlty constantly distributed along the Zero value**, which is suspicious regarding the expecte quality of the prediction. The residuals tend to decrease as the fitted `audience_score` increase.

For residuals vs `crew_score`, we observe an uniform distribution, which is conform to a good linear model.

For `imdb_num_votes`, the shape is due to the actual distribution of this predictors, which is very right skewed :

```{r}
ggplot(movies,aes(x=imdb_num_votes)) +geom_histogram()
```
Using an unbalanced predictor is not recommended, i exclude it then and recompute the model :

```{r}
lm3<- lm(audience_score ~ title_type + genre  + 
    best_pic_nom + title_score + crew_score, data = movies_simple)

best_lm<-step(lm3, direction = "backward", trace = 0)
summary(best_lm)
```
Finally, the model as 4 predictors : `genre`, `bestpic`, `title_score``and `crew_score`.

It can explain 40% of the variance, which is low. 
Intercept has no real meaning in the context. Genre like Documentary and Musical increase the audience_score of 20%, and one point of `title_score` gives 7.4 additional percent of  `audience_score`, one point of `crew_score` gives 3.8 percent of `audience_score`. The p-value allows us to reject the $H_{0}$ hypothese that the predictors have no effect on the `audience_score`.

```{r}

ggplot(data = best_lm, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals") + ggtitle("Residuals vs Fitted values")

ggplot(data = best_lm, aes(x = .resid)) +
  geom_histogram(binwidth = 2) +
  xlab("Residuals") + geom_vline(xintercept = median(best_lm$residuals),color='blue')
```

The residuals are shaped along a slightly left skewed normal distribution, as the blue line for median residuals shows.


```{r}
ggplot(data = best_lm, aes(sample = .resid)) +
  stat_qq()+ ggtitle("qqplot")
```

The previous observation is confirmed by the qqplot : the residuals follow a **slightly distorted normal distribution**.

We can therefore use the model to predict the `audience_score` of a movie out of the current dataset.

* * *

## Part 5: Prediction

For the prediction, i selected "Independance Day : resurgence", one of the worst movies of 2016.

![ ](independance_day_resurgence.png)

Before proceeding to the prediction, we have to compute the 2 new variables, `title_score` and `crew_score` :

```{r}
title<-"Independance Day Resurgence"
crew <- "Roland Emmerich Liam Hemsworth Jeff Goldblum Jessie T. Usher Bill Pullman Maika Monroe"

title_score <- str_count(tolower(title),paste(top_words, collapse='|')) - str_count(tolower(title),paste(worst_words, collapse='|'))

crew_score <- str_count(tolower(crew),paste(top_crew, collapse='|')) - str_count(tolower(title),paste(worst_crew, collapse='|'))

title_score
crew_score
```

Finally, we can predict the `audience_score` :
```{r}
independance_day_resurgence <- data.frame(genre = "Action & Adventure", runtime = 119, mpaa_rating = "PG-13",  critics_score = 31, best_any = "Yes",title_type="Feature Film",best_pic_nom='no',title_score=title_score,crew_score=crew_score)

predict(best_lm,newdata = independance_day_resurgence,interval  = "prediction")
```

So we get 60% `audience_score` estimation for an observed of 35%, that means a Root Mean Square Error of 25% !
The confidence interval is almost as wide as the `audience_score` range. That is a **very weak model**.

*Spoiler : i have not watched this movie.*


* * *

## Part 6: Conclusion

As the aim was to use the text elements of the dataset to make a prediction, the performance of the model, on one sample is bad. Two elements here :

- i removed predictors such as `imdb_rating`, 'critics_score` with **high predictive power**.
- the model created tends to **overfit** on the current dataset, due to intrinsic nature of the new variables.

Using words occurence as predictor was finally not a good idea, but as too often stated, with more data, this approach would have performed  better.
