---
title: "Peer Assessment II"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---

# Background

As a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.

# Training Data and relevant packages

In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.

```{r load, message = FALSE}
load("ames_train.Rdata")
```

Use the code block below to load any necessary packages

```{r packages, message = FALSE}
library(statsr)
library(dplyr)
library(tidyr)
library(stringr)
library(e1071)
library(ggplot2)
library(GGally)
library(ggcorrplot)
library(easyGgplot2)
library(gridExtra)
library(BAS)

```

## Part 1 - Exploratory Data Analysis (EDA)

When you first get your data, it's very tempting to immediately begin fitting models and assessing how they perform.  However, before you begin modeling, it's absolutely essential to explore the structure of the data and the relationships between the variables in the data set.

Do a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see. 

After you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).

* * *

### Target profil

How does the price distribution look like ?

```{r}
ggplot(ames_train,aes(x=price))+ geom_histogram(bins=30,fill='cyan4')
```

It is **right skewed** and monomodal. We might instead use the log value of price as target, to decrease the RMSE :

```{r}
ggplot(ames_train,aes(x=log(price)))+ geom_histogram(bins=30,fill='lightblue3')
```

Indeed, the skweness disappears.


### Features type
Let's check first the quantity and quality of the features :

```{r}
ncol(ames_train)
summarise_each(ames_train,funs(class)) %>% gather() %>% group_by(value) %>% summarise(count= n())
```
So we've got 80 potential predictors for the price variable, 43 *categorical* and 37 *continuous*.
It is a lot of features, we should reduce those figures.

### Missing data 

How many of those features are not really informative (ie high NA ratio) ?
For this i compute  the NA ratio per features and filter on a >50% threshold :

```{r}
df.high_na<- ames_train %>% summarise_each(funs(sum(is.na(.)))) %>% gather('Variable','Number.of.NA')  %>% mutate(ratio_perc=Number.of.NA/nrow(ames_train)*100) %>% filter(ratio_perc>50) %>% arrange(desc(ratio_perc)) 
df.high_na
high.na.features<- as.vector(df.high_na$Variable)
high.na.features
```
Hence we can drop `Pool.QC`, `Misc.Feature`, `Alley`, `Fence` as they not provide enough info for the overall dataset.


### Variables balance


Let's see which variables are not well balanced.

#### For continous variables

To proceed, we compute the skeweness for each continuous variable and select the **top 20 most skewed** :

Select the numeric features only :

```{r}
integer_features <- summarise_each(ames_train,funs(is.integer)) %>%gather()%>% filter(value==T & key!="price")
integer_features<- as.vector(integer_features$key)

categorical_features <- summarise_each(ames_train,funs(is.factor)) %>%gather()%>% filter(value==T )
categorical_features<- as.vector(categorical_features$key)
```

Compute the skeweness and select the most skewed :
```{r}

df_skew <-ames_train %>% select(one_of(integer_features)) %>% select(-one_of(high.na.features)) %>%summarise_each(funs(skewness=skewness)) %>% gather()%>% arrange(desc(abs(value))) %>% top_n(20)

df_skew
high_skewed_features<- as.vector(df_skew$key)
high_skewed_features<- vapply(strsplit(high_skewed_features,"_"), `[`, 1, FUN.VALUE=character(1))
```

We can now apply a logarithm transformation on the most skewed  continuous features :
```{r}
df_skewed_features_log<- ames_train %>% select_(.dots=high_skewed_features) %>%  mutate_each(funs(log(1+.)))
```

And finaly we plot those **logs values distributions** and see whether the variable distribution remains skewed :

```{r}

plot_data_column = function (data, column){
      p<-ggplot(data = data, aes_string(x = column)) +
        geom_histogram(fill = "lightgreen",binwidth = .1) +
        xlab(column)
}

myplots <- lapply(high_skewed_features, plot_data_column, data = df_skewed_features_log)


n <- length(myplots)
nCol <- floor(sqrt(n))
do.call("grid.arrange", c(myplots, ncol=nCol))

```

For the following features,the logarithmic transformation did improve the distribution :

- `Lot.Area`
- `area`
- `X1st.Flr.SF`
- `TotRms.AbvGrd`

For the other, we can exclude them for the scope.

```{r}
bad.balances.continous.features<- high_skewed_features[! high_skewed_features %in% c("Lot.Area","area","X1st.Flr.SF","TotRms.AbvGrd")]
bad.balances.continous.features
```

#### For categorical variables

As a measure of skeweness is not valid for categorical variables, we can plot their distribution directly :
```{r}

plot_data_column = function (data, column){
      p<-ggplot(data = data, aes_string(x = column)) +
        geom_bar(fill = "lightblue",stat="count") +
        theme(
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
        theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
        ylab("")
      
}

myplots <- lapply(categorical_features, plot_data_column, data = ames_train)

myplots_split <-  split(myplots, 1:2)

n <- length(myplots_split[[1]])
nCol <- floor(sqrt(n))
do.call("grid.arrange", c(myplots_split[[1]], ncol=nCol))

n <- length(myplots_split[[2]])
nCol <- floor(sqrt(n))
do.call("grid.arrange", c(myplots_split[[2]], ncol=nCol))
```

From the resulting distributions, we can exclude :

- `Alley`
- `Land.Contour`
- `Condition.2`
- `Roof.Matl`
- `Bsmt.Cond`
- `Heating`
- `Central.Air`
- `Garage.Cond`
- `Street`
- `Land.Slope`
- `Functional`
- `Utilities`

```{r}
bad.balances.discrete.features<- c("Utilities","Alley","Land.Contour","Condition.2","Roof.Matl","Bsmt.Cond","Heating","Central.Air","Garage.Cond","Street","Land.Slope","Functional","Utilities")

```
### Focus on some features 

Here are some feature i found interesting to focus on :

#### House Style :
```{r}
p0<-ggplot(ames_train,aes(x=reorder(House.Style, price, FUN=median),price,fill=House.Style))+ geom_boxplot()+theme(legend.position="none")
p1<-ggplot(ames_train,aes(x=reorder(House.Style, price, FUN=median),fill=House.Style))+ geom_bar(stat = 'count')+ theme(legend.position="none")

ggplot2.multiplot(p0,p1,cols = 1)
```

#### Exterior 1 : :
```{r}
p0<-ggplot(ames_train,aes(x=reorder(Exterior.1st, price, FUN=median),price,fill=Exterior.1st))+ geom_boxplot()+theme(legend.position="none")
p1<-ggplot(ames_train,aes(x=reorder(Exterior.1st, price, FUN=median),fill=Exterior.1st))+ geom_bar(stat = 'count')+ theme(legend.position="none")

ggplot2.multiplot(p0,p1,cols = 1)
```

#### Exterior 2 : :
```{r}
p0<-ggplot(ames_train,aes(x=reorder(Exterior.2nd, price, FUN=median),price,fill=Exterior.2nd))+ geom_boxplot()+theme(legend.position="none")
p1<-ggplot(ames_train,aes(x=reorder(Exterior.2nd, price, FUN=median),fill=Exterior.2nd))+ geom_bar(stat = 'count')+ theme(legend.position="none")

ggplot2.multiplot(p0,p1,cols = 1)
```

#### Mas.Vnr.Area :

```{r}

ggplot(ames_train,aes(x=log(Mas.Vnr.Area+1)))+geom_histogram()
ggplot(ames_train,aes(log(Mas.Vnr.Area+1),price))+ geom_jitter(colour='cyan4')+ geom_smooth(method = 'lm')
```

Indeed, those feature are worth trying in the model, but beware of the **multicollinearity.**


### Correlations

New that the features list got smaller, we compute the correlation for the remaining variables :

```{r}
features.to.exclude<- c(high.na.features, bad.balances.continous.features,bad.balances.discrete.features)
feature.of.interest<- colnames(ames_train)[!colnames(ames_train) %in% features.to.exclude]
```

We convert the categorical into numeric, **assuming the ranks do follow a logical order** (which might be false).
```{r}
ames_train_selected_features <- ames_train %>% select(one_of(c('price',feature.of.interest))) %>% mutate_if(is.factor,as.numeric) %>% na.omit()
df.correlation<- as.data.frame(cor(ames_train_selected_features))
df.correlation$feature_name<-rownames(df.correlation)

df.highest.corr.features<- df.correlation %>% select(feature_name,price) %>% arrange(desc(price)) %>% filter(feature_name!="price") %>% filter(abs(price)>0.4) %>% arrange(desc(price))

highest.corr.features<-as.vector(df.highest.corr.features$feature_name)
highest.corr.features
```



```{r}


corr <- round(cor(ames_train_selected_features %>% select(one_of(c("price",highest.corr.features)))),1)

# Plot
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 2, 
           method="square", 
           colors = c("cyan4", "white", "red3"), 
           title="Correlogram of Selected Features", 
           ggtheme=theme_bw)

```


We can clearly see that the features are almost all tightly correlated together, mean the problem of multicollinearity has to be taken into account. 



Using a method such as *step*, or a Bayesian prior could save us the tedious task of cleaning the collinear variables.


### New features : confort

On interesting festures might be a measurement of the comfort :
**the (Kitchens + BathRooms) vs total Room Rooms ratio ponderated by the surface.**

```{r}
ames_train$Built.Ratio<- (ames_train$Kitchen.AbvGr+ames_train$Full.Bath)*(ames_train$X2nd.Flr.SF)^2/ ames_train$Bedroom.AbvGr
ggplot(ames_train,aes(Built.Ratio,price))+ geom_jitter(colour='cyan4')+ geom_smooth(method = 'lm')

```



### Summary EDA

Finally, the 3 graphs that summarise at best the relationships in the dataset :

```{r creategraphs}
ggplot(ames_train,aes(Overall.Qual,log(price)))+ geom_jitter(colour='cyan4')+ geom_smooth(method = 'lm')+ ggtitle('1 - Overall House Quality vs. log(Price)')

ggplot(df.highest.corr.features,aes(x=reorder(feature_name, price,FUN = abs),y=price,fill='cyan4'))+  geom_bar(stat="identity")+theme(legend.position="none",axis.text.x=element_text(angle=45, hjust=1),                                                                        axis.text.y=element_text(angle=45, hjust=1))  + ylab('Pearson correlation with price')  + ggtitle('2 - Highest correlation with price')                     

ggplot(ames_train,aes(Exter.Qual,Overall.Qual))+ geom_jitter(colour='cyan4')+ geom_smooth(method = 'lm')+ ggtitle('3 - Example of collinerarity')

```

The last chart (**Exter.Qual vs log(price)**)reminds us how tricky the features selection is : although i did considere this feature as well balanced and it ranks as 2nd in term of correlation with price, we have to admit the **lack of linearity with log(price)**


* * *

## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis

### Section 2.1 An Initial Model
In building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is **not** to identify the "best" possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.

Based on your EDA, select *at most* 10 predictor variables from “ames_train” and create a linear model for `price` (or a transformed version of price) using those variables. Provide the *R code* and the *summary output table* for your model, a *brief justification* for the variables you have chosen, and a *brief discussion* of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).

* * *

Given the resulf of the EDA, i would instinctively select the following features to make a simple but robust model :

- `Overall.Qual` and `Exter.Qual` : the best $\rho$
- `area` and `Lot.Area`: it seems obvious that price is related to the area.
- `Year built` : *ibid.*


```{r fit_model}

model1<-lm(price~Overall.Qual+area+Year.Built,data=ames_train)
summary(model1)
```
We resulting model score quite well with an adjusted $R^2=0.7456$. 

What does it mean ?

It means the model we trained managed to explain  74% of the overall variance, keeping in mind that a $R^2 =1$ would result in explaining fully this variance.

The three predictors having a positive coefficient, each increase of their value with push the house price higher.

Regarding the p-value, that provide evidence of the real influence of the predictor on the price, they are all well below a reasonable significance level. 



* * *

### Section 2.2 Model Selection

Now either using `BAS` another stepwise selection procedure choose the "best" model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?

* * *

Now it's time to use the *step* method ! We first use the **AIC** (Aikike Informatoin criterion), then the **BIC** (Bayesian Information Criterion)

```{r model_select}

model.AIC<-step(model1,k=2,trace=F)
model.BIC<-step(model1,k=log(nrow(ames_train)),trace=F)

summary(model.AIC)
summary(model.BIC)
```

Due to the limited number of predictors (3), both model selection methods did reach the same model : no feature was considered worth excluding.
* * *

### Section 2.3 Initial Model Residuals
One way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.

* * *

I use the BIC model (less predictors, slightly lower $R^2$) to plot the residuals :

```{r model_resid}
ggplot(model.BIC,aes(x=.fitted,y=.resid))+geom_point(colour='cyan4',alpha=0.3)+ geom_smooth(method='loess')+ggtitle('prediction vs residual for Training set')
```

What we observe :

- the overall distribution is **somehow** uniform.
- Remember the **Very good Deal** ? The outlier from Week 4 is still here ! 
- for low price **<150K**, the model tends to **under-estimate** the price.
- **between 150K and 250K** : the opposite occurs : price are **over-evaluated** by the model.
- beyond 250K : under-evaluation.
- past 300K the model can hardly provide reasonable estimate of the price.

* * *

### Section 2.4 Initial Model RMSE

You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).

* * *


To compute the Root Mean Squared Error, i we use the standard formula :

$RMSE = \sqrt{\frac{\sum_{i,n}(estimate_{i}-observed_{i})^2}{n}}$

But as we are estimating the $log$ value :

$RMSE = \sqrt{\frac{\sum_{i,n}(\exp^{estimate_{i}}-observed_{i})^2}{n}}$

```{r model_rmse}

ames_train$prediction<-predict(model.BIC,ames_train)

RMSE<-sqrt(mean((ames_train$prediction -ames_train$price)^2,na.rm=TRUE))
RMSE
```

This model result in a RMSE of **41232.49$**.


* * *

### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.

```{r loadtest, message = FALSE}
load("ames_test.Rdata")
```

Use your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?

* * *



```{r initmodel_test}
ames_test$Built.Ratio<- (ames_test$Kitchen.AbvGr+ames_test$Full.Bath)*ames_test$X2nd.Flr.SF/ ames_test$Bedroom.AbvGr

ames_test$prediction<-predict(model.BIC,newdata=ames_test)
ames_test$resid<-ames_test$price-ames_test$prediction

RMSE<-sqrt(mean((ames_test$resid)^2,na.rm=TRUE))
RMSE


ggplot(ames_test,aes(x=prediction,y=resid)) +geom_point(colour='cyan4',alpha=0.3)+ geom_smooth(method='loess')+ggtitle('prediction vs residual for Test set')

```

The RMSE is **astonishly lower**, which is **not was we would expect from a model**.
My guess is there are **less outliers than in the training set**, and the split between training and testing has not been randomly done.

* * *

**Note to the learner:** If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.

## Part 3 Development of a Final Model

Now that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.  

Carefully document the process that you used to come up with your final model, so that you can answer the questions below.

### Section 3.1 Final Model

Provide the summary table for your model.

* * *


- `Overall.Qual` and `Exter.Qual` : the best $\rho$
- `area` and `Lot.Area`: it seems obvious that price is related to the area.
- `Year built` : *ibid.*
- `Built.Ratio` : A new features created during the EDA.
- `Bsmt.Fin.Type.1` : one basement characteristics to widen the heterogeneity of the features.
- `Exterior.1st` : *ibid.*
- `Garage.Cars` : In the US, cars tends to influence the comsumer behaviour.
- `Kitchen.Qual` : If for other countries such as mine (FR), it weights on the buying decision.
- `Neighborhood` : from Week 4 assignement, this feature had an impact on the price. My EDA did not focused on this kind of categorical feature, but i'd like to give it a chance.


Again, we bernchmark three different models selections techniques :

- step with AIC,
- step with BIC,
- Bayesian based model selection with a Markov Chain Monte Carlo sampling method and a ZS-null prior.


#### step with AIC
```{r model_playground}

model2<-lm(log(price)~Overall.Qual+Exter.Qual+log(area)+log(Lot.Area)+Year.Built+Built.Ratio+BsmtFin.Type.1+Exterior.1st+Garage.Cars+Kitchen.Qual+Neighborhood,data=ames_train)

best.model2.AIC<-step(model2,trace=F)
summary(best.model2.AIC)


```

#### Step with BIC
```{r}

best.model2.BIC<-step(model2,trace=F,k=log(nrow(ames_train)))
summary(best.model2.BIC)


```


#### Bayesian method with bas package 
```{r}
# best.model2.BAS<- bas.lm(log(price)~Overall.Qual+Exter.Qual+log(area)+log(Lot.Area)+Year.Built+Built.Ratio+BsmtFin.Type.1+Exterior.1st+Garage.Cars+Kitchen.Qual+Neighborhood,data=ames_train, prior = "ZS-null",
#                    modelprior = uniform(),
#                    method = 'MCMC')
# 
# summary(best.model2.BAS)
```


#### Model evaluation on the testing set :

Now we can compute the RMSE of the three models on the testing set :


#### Step AIC :
```{r}

# There are new levels on the testing set for for Exterior.1st and Neighborhood, i add them in the model.
best.model2.AIC$xlevels[["Exterior.1st"]] <- union(best.model2.AIC$xlevels[["Exterior.1st"]], levels(ames_test$Exterior.1st))
best.model2.AIC$xlevels[["Neighborhood"]] <- union(best.model2.AIC$xlevels[["Neighborhood"]], levels(ames_test$Neighborhood))


ames_test$prediction2.AIC<-exp(predict(best.model2.AIC,newdata=ames_test))
ames_test$resid2.AIC<-ames_test$price-ames_test$prediction2.AIC

RMSE<-sqrt(mean((ames_test$resid2.AIC)^2,na.rm=TRUE))
RMSE


ggplot(ames_test,aes(x=prediction2.AIC,y=resid2.AIC)) +geom_point(colour='cyan4',alpha=0.3)+ geom_smooth(method='loess')+ggtitle('prediction vs residual for Test set')

```

#### Step BIC :
```{r}

# No need to adjust the levels for Exterior.1st and Neighborhood, they are not part of the BIC model.
ames_test$prediction2.BIC<-exp(predict(best.model2.BIC,newdata=ames_test))
ames_test$resid2.BIC<-ames_test$price-ames_test$prediction2.BIC

RMSE<-sqrt(mean((ames_test$resid2.BIC)^2,na.rm=TRUE))
RMSE


ggplot(ames_test,aes(x=prediction2.BIC,y=resid2.BIC)) +geom_point(colour='cyan4',alpha=0.3)+ geom_smooth(method='loess')+ggtitle('prediction vs residual for Test set')

```

#### Bayesian method :
```{r}
# There are new levels on the testing set for for Exterior.1st and Neighborhood, i had them in the model.
# best.model2.BAS$xlevels[["Exterior.1st"]] <- union(best.model2.BAS$xlevels[["Exterior.1st"]], levels(ames_test$Exterior.1st))
#   best.model2.BAS$xlevels[["Neighborhood"]] <- union(best.model2.BAS$xlevels[["Neighborhood"]], levels(ames_test$Neighborhood))
# 
# test_BAS<- copy(best.model2.BAS)
# 
# ames_test$prediction2.BAS<-exp(predict.bas(best.model2.BAS,newdata=ames_test, estimator="HPM"))
# ames_test$resid2.BAS<-ames_test$price-ames_test$prediction2.BAS
# 
# RMSE<-sqrt(mean((ames_test$resid2.BAS)^2,na.rm=TRUE))
# RMSE
# 
# 
# ggplot(ames_test,aes(x=prediction2.BAS,y=resid2.BAS)) +geom_point(colour='cyan4',alpha=0.3)+ geom_smooth(method='loess')+ggtitle('prediction vs residual for Test set')

```

Well, i could not make the BAS predict work on the testing set, due to new factors introduced in the testing set.
R, BAS are up to date. I posted it on the forum Week8.

* * *

### Section 3.2 Transformation

Did you decide to transform any variables?  Why or why not? Explain in a few sentences.

* * *


Some of the predictor will be use with **their log version**, as we get a better balanced distribution as seen in the EDA.

```{r model_assess}
```

* * *

### Section 3.3 Variable Interaction

Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.

* * *

I used the `Built.Ratio` which is a combination of quantitative features, as described in the EDA.

```{r model_inter}
```

* * *

### Section 3.4 Variable Selection

What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.

* * *

After a benchmark on three methods describe above, i decided to use the step BIC approach, which is a trade-off between a good $R^2=0.86$ and low Degrees of Freedom (20).


```{r model_select}
```

* * *

### Section 3.5 Model Testing

How did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.

* * *

On the testing set, the Step BIC performed much better than the step AIC in term of RMSE.

```{r model_testing}
```

* * *

## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

For your final model, create and briefly interpret an informative plot of the residuals.

* * *

```{r}
ggplot(ames_test,aes(x=prediction2.BIC,y=resid2.BIC)) +geom_point(colour='cyan4',alpha=0.3)+ geom_smooth(method='loess')+ggtitle('prediction vs residual for Test set')
```

The residuals vs fitted values is not really uniform, that means the model is not optimal.


* * *

### Section 4.2 Final Model RMSE

For your final model, calculate and briefly comment on the RMSE.

* * *


```{r}

ames_train$prediction2.BIC<-exp(predict(best.model2.BIC,ames_train))

RMSE.train<-sqrt(mean((ames_train$prediction2.BIC -ames_train$price)^2,na.rm=TRUE))
RMSE.train         
RMSE.test<-sqrt(mean((ames_test$resid2.BIC)^2,na.rm=TRUE))
RMSE
```


The RMSE on the testing set is still lower than the training set, which is suspicious. Was the split really random ?

* * *

### Section 4.3 Final Model Evaluation

What are some strengths and weaknesses of your model?

* * *

The model as not an uniform residual distribution, meaning the predictive power is not stable along the different prices.
I would definitively remote the outliers from the training model. 
Still, the model has a good R2 (0.85) with limited predictors (20).
Pity i could not perform the Bayesian prediction using Neighborhoof and Exererior.1.

* * *

### Section 4.4 Final Model Validation

Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. 

You will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention:
* What is the RMSE of your final model when applied to the validation data?  
* How does this value compare to that of the training data and/or testing data?
* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?  
* From this result, does your final model properly reflect uncertainty?

```{r loadvalidation, message = FALSE}
load("ames_validation.Rdata")
```

* * *



```{r model_validate}
# Add the new feature :
ames_validation$Built.Ratio<- (ames_validation$Kitchen.AbvGr+ames_validation$Full.Bath)*(ames_validation$X2nd.Flr.SF)^2/ ames_validation$Bedroom.AbvGr

ames_validation$prediction2.BIC<-exp(predict(best.model2.BIC,newdata=ames_validation))

RMSE.validation<-sqrt(mean((ames_validation$prediction2.BIC -ames_validation$price)^2,na.rm=TRUE))
RMSE.validation    

predict.full <- exp(predict(best.model2.BIC, newdata=ames_validation, interval = "prediction", level = 0.95))
coverage.prob.full <- mean(ames_validation$price > predict.full[,"lwr"] & ames_validation$price < predict.full[,"upr"],na.rm=T)
coverage.prob.full
```

The RMSE on the validation set using the best.model2.BIC is even lower than for the testing and training set.
Again, the sampling on training set has not been properly randomized, with a high proportion of outliers.

With 97% of the observed values fitting inside the 95% prediction confidence interval, the model does perform quite well.



* * *

## Part 5 Conclusion

Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. 

* * *

Features selection was an interesting challenge, with lots of correlated features.
A **Principal Components Analysis** would have also been worth investigate, though we would have lose the ability to interpret the transformed predictors.


I do remain doubtful on the content of the training set, which performs worst than both testing and validation set regarding the RMSE. **Eliminating the outliers** would have really improved the predictive power of the model.

Pity i **could not use the Bayesian model** to compute a prediction, which looked promising regarding the $R^2$ value.


### Regression with PCA (PCR)

```{r}

library(mice)
ames_train <-  ames_train%>% select(one_of(feature.of.interest))%>% mutate_if(is.factor,as.numeric)
ames_test <-ames_test %>% select(one_of(feature.of.interest))%>% mutate_if(is.factor,as.numeric)

ames_train <- mice::complete(mice(ames_train,m=1, maxit=500, method='cart', seed=500))
ames_test <- mice::complete(mice(ames_test, m=1, maxit=500, method='cart', seed=500))

pcr_model <- pcr(price~., data = ames_train, scale = TRUE, validation = "CV")
summary(pcr_model)

# Plot the root mean squared error
validationplot(pcr_model)


# Plot the R2
validationplot(pcr_model, val.type = "R2")


```

```{r}
ames_test$pcr_pred <- predict(pcr_model, ames_test , ncomp = 20)

ames_test$resid_pcr<-(pcr_pred - ames_test$price)^2
sqrt(sum(mean(ames_test$resid_pcr,na.rm=T)))
ggplot(ames_test, aes(x=pcr_pred,y=resid_pcr)) +geom_point(colour='cyan4',alpha=0.3)+ geom_smooth(method='loess')+ggtitle('prediction vs residual for Test set')

```


* * *
