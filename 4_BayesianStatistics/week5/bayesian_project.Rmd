---
title: "Bayesian modeling and prediction for movies"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(statsr)
library(BAS)
library(lubridate)
library(ggcorrplot)
source('multiplot.r')
```

### Load data



```{r load-data}
load("movies.Rdata")
str(movies)
```


* * *

## Part 1: Data

*Describe how the observations in the sample are collected, and the implications of this data collection method on the scope of inference (generalizability / causality).*


This dataset, as described in the instructions does consist in a **random sample** of **651 US movies** extracted from both **IMDB** and **Rotten Tomatoes** databases.
```{r}
summary(movies$thtr_rel_year)
ggplot(data = movies, aes(x = thtr_rel_year))+ geom_bar() + ggtitle("Theater release year of dataset")
```

All have been released between 1970 and 2014. 

Assuming this is an observational study with random sampling, it should be *representative* of US movies. By using random sampling, the resulting prediction model should be *generalizable* to US movies released between 1970 and 2014.




#### Remark regarding generalizability *(out of scope)*

In order to get an idea whether the random sample is generalizable, i compared it with a larger IMDB titles dataset ( *imdb_official_titles.csv* 236,710 different movies) and focused on the **release year distribution**.

I filtered this imdb dataset on the same time period as the movies dataset :

```{r}

min_tht_release_year <- min(movies$thtr_rel_year)
max_tht_release_year<- max(movies$thtr_rel_year)
imdb_titles<-read.csv('imdb_official_titles.csv')
sample_imdb_titles<-sample_n(imdb_titles[(imdb_titles$year>=min_tht_release_year) &
 (imdb_titles$year<=max_tht_release_year),],nrow(movies))

both_datasets<-data.frame(movies$thtr_rel_year,sample_imdb_titles$year)
colnames(both_datasets) <- c("movies","imdb_sample")
both_datasets<-both_datasets %>% gather('dataset','release_year')

ggplot(data=both_datasets,aes(x=release_year,fill=dataset))+geom_histogram(position="identity",alpha=0.3)

```


Both *movies* and *imdb sample* datasets are not normally distributed as shown with the test below :
```{r}
shapiro.test(movies$thtr_rel_year)
shapiro.test(sample_imdb_titles$year)
```
I then  used the Kolmogorvo-Smirnov test to check if both theater release years are equally distributed.

```{r}
ks.test(movies$thtr_rel_year,sample_imdb_titles$year)
```
A qqplot does confirm this result :
```{r}
qqplot(movies$thtr_rel_year,sample_imdb_titles$year)
```

Although the *movies* dataset is a random sample, i cast doubts on the generalizability of such sample.


#### Causality
As we are in a context of **observational studies**, we might conclude some association patterns, but contrary to an experimental study with random samples,  **no causality** that might be deduced from it.


* * *

## Part 2: Data manipulation

*Create new variables using the mutate function in the dplyr package following these guidelines:*

*Create new variable based on `title_type`:*
*- New variable should be called `feature_film` with levels yes (movies that are feature films) and no (2 pt)*
 
```{r}
movies<-movies %>% mutate(feature_film = if_else(movies$title_type=='Feature Film','yes','no') ) %>% mutate(feature_film = as.factor(feature_film))
```

*Create new variable based on `genre`: *
*- New variable should be called `drama` with levels yes (movies that are dramas) and no (2 pt)*
```{r}
movies<-movies %>% mutate(drama = if_else(movies$genre=='Drama','yes','no'))%>% mutate(drama = as.factor(drama))
```

*Create new variable based on `mpaa_rating`: *
*- New variable should be called `mpaa_rating_R` with levels yes (movies that are R rated) and no (2 pt)*
```{r}
movies<-movies %>% mutate(mpaa_rating_R = if_else(movies$mpaa_rating=='R','yes','no'))%>% mutate(mpaa_rating_R = as.factor(mpaa_rating_R))
```


*Create two new variables based on `thtr_rel_month`:*
*- New variable called `oscar_season` with levels yes (if movie is released in November, October, or December) and no (2 pt)*
```{r}
movies<-movies %>% mutate(oscar_season = if_else(movies$thtr_rel_month>9,'yes','no'))%>% mutate(oscar_season = as.factor(oscar_season))
```

*- New variable called `summer_season` with levels yes (if movie is released in May, June, July, or August) and no (2 pt)*
```{r}
movies<-movies %>% mutate(summer_season = if_else((movies$thtr_rel_month>4)&(movies$thtr_rel_month<9),'yes','no'))%>% mutate(summer_season = as.factor(summer_season))
```




* * *

## Part 3: Exploratory data analysis

*Perform exploratory data analysis (EDA) of the relationship between audience_score and the new variables constructed in the previous part. Your EDA should contain numerical summaries and visualizations. This might mean you initially create a lot more visualizations and summary statistics than what you finally choose to include in your paper. Each R output and plot should be accompanied by a brief interpretation.*



Let's store the newly created variables in a vector :

```{r}
new_vars = c('feature_film','drama','mpaa_rating_R','oscar_season','summer_season')
```


We can draw a correlogramm of the selected variables :

```{r}

movies_selected_var<- movies %>% select(one_of(new_vars)) %>% mutate_each(funs(ifelse(. == 'no', 0,1)))
movies_selected_var$audience_score <- movies$audience_score


corr <- round(cor(movies_selected_var),3)
ggcorrplot(corr, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlogram of Selected variables", 
           ggtheme=theme_bw)
```

+ The *feature_film* variable is the most correlated (negatively) with our target parameter *audience_score*.
+ *summer_season* and *oscar_season* are quite correlated (Pearson = -0.44), which could produce **multicollinearity.**
+ There is a slight positive correlation between *drama* and *audience_score.*



Then, let's create a standard function to compute the comparison between *audience_score* and the newly created variables :
```{r}

compute_relationship <- function(movies,new_var){
  p0<-ggplot(data=movies,aes_string(y="audience_score",x=new_var,col=new_var)) + geom_boxplot() + theme(legend.position="none")
  groupy<-movies %>% group_by_("thtr_rel_year",new_var) %>% summarise(average_audience_score=mean(audience_score))
  p1<-ggplot(groupy,aes_string(x="thtr_rel_year",y="average_audience_score",col=new_var))+ geom_point()
  
  p2<-ggplot(data=movies,aes_string(x="audience_score",fill=new_var))+geom_histogram(bins=100,position="identity",alpha=0.5)+ guides(fill=FALSE)
  layout <- matrix(c(1,3,2,2), nrow = 2, byrow = TRUE)
  multiplot(p0, p1,p2, cols=2,layout=layout)
}

```


We focus on the new variables with an absolute Pearson correlation **higher than 0.1**.

#### feature_film

```{r}
compute_relationship(movies,"feature_film")
```

+ We notice that the non-feature film are almost all above the the average_score, and regardless the years.
+ This confirms the relatively high correlation between this variable and audience_score. 
+ It will be interesting to see the posterior probability of inclusion of this variable in the top 5 regression models.


#### drama

```{r}
compute_relationship(movies,"drama")
```

For drama movies, though above the others genre, the average audience_score is decreasing as years go by. 

* * *

## Part 4: Modeling

*Develop a Bayesian regression model to predict `audience_score` from the following explanatory variables. Note that some of these variables are in the original dataset provided, and others are new variables you constructed earlier:`feature_film`,` drama`, `runtime`, `mpaa_rating_R`, `thtr_rel_year`, `oscar_season`, `summer_season`, `imdb_rating`, `imdb_num_votes`, `critics_score`, `best_pic_nom`, `best_pic_win`, `best_actor_win`, `best_actress_win`, `best_dir_win`, `top200_box`. Complete Bayesian model selection and report the final model. Also perform model diagnostics and interpret coefficients of your final model in context of the data.*

First, we generate a new dataset based on the listed variables :
```{r}
features<- c("audience_score", "feature_film","drama", "runtime", "mpaa_rating_R", "thtr_rel_year", "oscar_season", "summer_season", "imdb_rating", "imdb_num_votes", "critics_score", "best_pic_nom", "best_pic_win", "best_actor_win", "best_actress_win", "best_dir_win", "top200_box")

movies_with_features<- movies %>% select(one_of(features))
```

Let's plot a correlogram of the selected features :
```{r}
movies_all_numeric <- movies_with_features %>% mutate_if(is.factor,as.numeric) %>% na.omit()

corr <- round(cor(movies_all_numeric),2)
ggcorrplot(corr, hc.order = FALSE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 2, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlogram of all features", 
           ggtheme=theme_bw)
```

Without a model, we can observe that the heavy predictors for audience_score are unsurprisingly `imdb_rating` (0.86) and `critics_score` (0.7)
They dwarf all the newly variables.


let's start with a Bayesian regression model using all selected features :
I use the **Zellner-Siow Cauchy prior** for this model, with a **Markov chain Monte Carlo** method.

```{r}
movies_no_na = na.omit(movies_with_features)
movies.ZS = bas.lm(audience_score ~ . -audience_score, data = movies_no_na,
                   prior = "ZS-null", 
                   modelprior = uniform(),
                   method = 'MCMC')
movies.ZS
```

The result above does confirm our assumption : no model without imdb_rating and critics_score.


We can now check whether the Markov chain Monte Carlo simulation has runned long enough :
```{r}
diagnostics(movies.ZS)
```

Yes, indeed, for both plots we get a line : the MCMC simulation did run long enough.



We can see the top 5 models selected by our Bayesian regressor :
```{r}
summary(movies.ZS)
```
The resulting determination coefficients R2 are quite good, above 0.75, which is not a suprise given the strongly correlated predictors **imdb_rating** and **critics_score**.

Amongst the newly created variable, only *mpaa_rating_R* has been selected twice within the top 5 models.


Now, let's proceed to a step by step analysis of the generated model :

First, we plot the spread of the residuals along the audience_score :

```{r}
plot(movies.ZS,which=1,add.smooth=T)
```

What we notice here are two elements :

- the movies with an effective low audience (from 0% to 35%) score are over-rated by the model.

- from 40% to 70% the opposite occurs.


That's **not a good result**, residuals should be uniformly spread. We should considere as outliers the low-rated films.



```{r}
plot(movies.ZS,which=2,add.smooth=T)
```

After 2000 models built, we reach a plateau, and no more probability is significantly added.

```{r}

plot(movies.ZS,which=3,add.smooth=T)
```

The best models (i.e with the highest marginal likelihood) are in the range of 2 to 4 predictors.

```{r}

plot(movies.ZS,which=4,add.smooth=T)
```

Finally, this plot confirms our first assumption : only imdb_rating and critics_score are relevant predictors for audience_score.

```{r}
image(movies.ZS,rotate = F)
```

Again, almost all best ranking models use  `imdb_rating` and `critics_score`.



We can have a look at the mean coefficient of the models :

```{r}
coef(movies.ZS)
```

Ranking first in term of posterior probability :

- `imdb_rating` (with a positive coefficient), 

- `critics_score` (with a positive coefficient),

- `runtime` (with a negative coefficient),

- `feature_film` (with a negative coefficient).

The better the critics are (sounds obvious), the shorter the movie is (runtime and feature_film), the better we can expect a good audience score.



* * *

## Part 5: Prediction


I choose the oscar winning movie Moonlight, with a **81**% Rotten Tomatoes audience score.

![ ](moon.jpg)




As i did not managed to use a new dataframe in the BAS predict method, i combined both *movies* and the testing dataframe.


```{r}
moonlight_df<-data.frame('runtime'=110, 'imdb_rating'=7.6,'critics_score'=.97, "feature_film"=as.factor('yes'),"drama"=as.factor('yes'),  "mpaa_rating_R"=as.factor('no'), "thtr_rel_year"=2016, "oscar_season"=as.factor('no'), "summer_season"=as.factor('no'), "imdb_num_votes"=115403,  "best_pic_nom"=as.factor('yes'), "best_pic_win"=as.factor('no'), "best_actor_win"=as.factor('no'), "best_actress_win"=as.factor('no'), "best_dir_win"=as.factor('no'), "top200_box"=as.factor('no'),'audience_score'=0)

df_with_prediction<- rbind(movies_with_features,moonlight_df,moonlight_df)
```

I then predict the audience_score using : 

+ Bayesian Model Average (*BMA*) to get the best prediction under squared error loss :

```{r}

movies.coef.ZS = coef(movies.ZS)

BMA = predict(movies.ZS, newdata=tail(df_with_prediction,1), estimator="BMA")
print('BMA')
BMA$fit
```



+ and Highest Probability Model (*HPM*) as benchmark :

```{r}

HPM = predict(movies.ZS, newdata=tail(df_with_prediction,2), estimator="HPM")
print('HPM')
HPM$fit

```

Let's have a look at the best model's predictors  :

```{r}

(movies.ZS$namesx[HPM$bestmodel +1])[-1]

```




With fitted value of *75.9754%* for *BMA* and 7*4.4379%* for *HPM*, we are 5% less than the observed value.
For HPM, the best model used `imdb_rating` and unsurprisingly, `critics_score`.


* * *

## Part 6: Conclusion

*A brief summary of your findings from the previous sections without repeating your statements from earlier as well as a discussion of what you have learned about the data and your research question. You should also discuss any shortcomings of your current study (either due to data collection or methodology) and include ideas for possible future research.*


The aim of this project was to find the best prediction model for audience_score. Even by creating new variables extracted from the current dataset, we could not beat the predictive strength of both `imdb_rating` and `critics_rating`, which, per se, are the closest features in term of correlation to our target `audience_score`.

I found worring the heterogenous spread of residuals for fitted `audience_score`, which implies the dataset might be first purged from low-rated movies, and that i did an error either during the data preparation or the model definition.

This is confirmed by the fact that a *simple linear regression model* predicts an audience_score of *80.9%* for Moonlight (81% observed audience_score).


