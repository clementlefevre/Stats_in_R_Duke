\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{verbatim}
\usepackage[T1]{fontenc}
\usepackage{tikz,overpic}
\usetikzlibrary{fit,shapes.misc}

\def\No{\textsf{N}}
\def\Ga{\textsf{Ga}}
\def\BF{\textit{BF}}
\def\n0{n_0}
\newcommand{\St}{\textsf{t}}
\newcommand{\E}{\textsf{E}}

\def\data{\text{data}}
\newcommand{\iid}{\ensuremath{\mathrel{\mathop{\sim}\limits^{\rm iid}}}}
\def\handwriting{\fontencoding{T1}\fontfamily{augie}\selectfont}

\title{Optional Reading:\\ Derivations for comparing two paired means using Bayes factors}

\author{Dr.~Merlise Clyde}
\date{Draft}


\usepackage{Sweave}
\begin{document}
\input{4-3-2-pair-notes-concordance}


\maketitle





%\begin{beamercolorbox}[rounded=true,shadow=true,wd=.9\textwidth]{ExBox}
%A SurveyUSA poll on parents perceptions of bullying of their children
%\end{beamercolorbox}



\section*{Paired Data}
In the example in the video,  we have $n = 10$ paired observations $Y_{iB}$ and $Y_{iS}$  for $i = 1, \ldots, n$ representing the concentrations of zinc at the bottom and surface, respectively.



Rather than working with the two groups of observations, we will work with the differences $D_{i} \equiv Y_{iB} - Y_{iS}$ to make inference about the difference in the means $\mu_1 - \mu_2 \equiv \mu$ converting this problem to a one group Normal problem.   

\begin{Schunk}
\begin{Sinput}
> zinc
\end{Sinput}
\begin{Soutput}
   bottom surface difference
1   0.430   0.415      0.015
2   0.266   0.238      0.028
3   0.567   0.390      0.177
4   0.531   0.410      0.121
5   0.707   0.605      0.102
6   0.716   0.609      0.107
7   0.651   0.632      0.019
8   0.589   0.523      0.066
9   0.469   0.411      0.058
10  0.723   0.612      0.111
\end{Soutput}
\end{Schunk}

We will make the same assumptions about the distributions of the differences as in the case of the frequentist paired t-test.  That is conditional on the parameters $\mu$ and $\sigma^2$
the observed differences are independently and identically distributed from a  normal distribution expressed  as
$$D_i \mid \mu, \sigma^2 \iid \No(\mu, \sigma^2).$$  To check the assumption of normality we can look at a histogram or normal quantile plot of the sampled differences. 

\includegraphics{4-3-2-pair-notes-hist}

\subsection*{Likelihood}
The normal sampling model leads to a likelihood function 
\begin{equation}
{\cal{L}}(\mu, \sigma^2) = \prod_{i = 1}^{n} \frac{1}{\sqrt{\sigma^2 2 \pi}} \exp{\left( - \frac{1}{2}\frac{(D_i - \mu)^2}{\sigma^2} \right)} 
\label{eq:lik}
\end{equation}
where the likelihood function is proportional to the sampling distribution of the data.
To simplify our calculations we can reduce the data down to two "sufficient" statistics, where
$$ \bar{D} \mid \mu, \sigma^2 \sim \No(\mu,  \sigma^2/n)$$
and is independent of 
$$ 
s^2 \mid \sigma^2 \sim  \Ga\left(\frac{n - 1}{2},  \frac{n-1}{2 \sigma^2}\right)
$$
where $s^2$ is the sample variance, $s^2 = \sum(D_i - \bar{D})^2/(n-1)$, and $\Ga$ is a gamma distribution.  Note, we will use the rate parameterization of the gamma, so if $Y \sim \Ga(a, b)$ then $Y$ has a probability density function 
$$
p(y) = \frac{1}{\Gamma(a)} b^a y^{a - 1} e^{- y b} 
$$
with expected value $a/b$.  From this we can find that $\E[s^2] = \sigma^2$ so that the sample variance is an unbiased estimator of the population variance. {\it Note that the rate parameterization that we are using here is different from the scale parameterization that is used in Week 2 for the Conjugate Poisson-Gamma}.  The rate parameterization leads to easier updating rules as we will see.   


For ease of derivation, we are going to create a new parameter $\phi \equiv 1/\sigma^2$ to help with specifying a conjugate prior distribution.  The parameter $\phi$ is known as the precision;  if the variance is small we have high precision, while if the variance is small we have more uncertainty  and low precision.  In the new parameterization our two statistics have sampling distributions

\begin{align}
 \bar{D} \mid \mu, \phi & \sim \No\left(\mu,  1/(\phi n)\right) \\
 s^2_d \mid \phi & \sim  \Ga(\nu/2,  \nu \phi/2 )
\end{align}
where $\nu = n-1 $ is the usual degrees of freedom leading to a likelihood function based on taking the product of the independent distributions 
$$
{\cal{L}}(\mu, \phi) \propto (n\phi)^{1/2} \frac{1}{\sqrt{(2 \pi)}} 
\exp\left\{- \frac 1 2  n \phi (\bar{D} - \mu)^2 \right\}
\frac{1 }{\Gamma(\nu/2)} \left(\frac{\nu \phi}{2}\right)^{\nu/2} {s^2_d}^{\nu/2 - 1} \exp{- \frac{\phi \nu s^2_d}{2}}.
$$

Note:  you could just start with the independent normal samples and through some algebra rearrange to get to this.

\section*{Conjugate Normal-Gamma Prior Distribution}


For Bayesian inference we need to assign prior distributions to all of the unknown parameters under all hypotheses. As a first attempt, conjugate prior distributions are a convenient choice or as we will encounter later provide building blocks for more complex distributions. Recall a conjugate prior distribution is one where the posterior distribution and the prior distribution are in the same family. 

\subsection*{Conjugate Prior and Posterior for $\mu$ given $\phi$}

In Week 2 we studied the conjugate prior for a normal mean assuming that $\sigma^2$ or $\phi$ was known.   While in this case the variance is unknown, 
conditional on $\sigma^2$ (or $\phi$ now), the conjugate prior for $\mu$ given $\phi$ is a normal distribution,  
$$ \mu \mid \phi \sim \No \left(m_0, \frac{1}{n_0 \phi} \right)
$$
where $m_0$ is the prior mean and $n_0$ is a hyper-parameter that is used to represent
how concentrated or less concentrated the distribution is about $m_0$ relative to the precision $\phi$, and may be
thought of as a prior imaginary sample size upon which the prior distribution is 
based if there are no historical observations. Taking $n_0 = 1$ implies that our prior distribution is worth the 
equivalent of one observation.  

Bayes theorem in proportional form leads to 
\begin{align}
p(\mu \mid \phi, \data)  \propto & {\cal{L}}(\mu, \phi) p(\mu \mid \phi) \\
& =  (n\phi)^{1/2} \frac{1}{\sqrt{(2 \pi)}} 
\exp\left\{- \frac 1 2  n \phi (\bar{D} - \mu)^2 \right\}
p(s^2 \mid \phi) \\
& \cdot
(n_0\phi)^{1/2} \frac{1}{\sqrt{(2 \pi)}} \exp\left\{- \frac 1 2  n_0 \phi (\mu - m_0)^2 \right\}
\intertext{where we have left the sampling distribution for $s^2$ as a density as it does not involve $\mu$. Ignoring constants that do not involve $\phi$ or $\mu$ we may simplify further} \\
p(\mu \mid \phi, \data) & \propto  \phi^{1/2}  \exp\left\{- \frac 1 2  n \phi (\bar{D} - \mu)^2  - \frac 1 2  n_0 \phi (\mu - m_0)^2 \right\}  \left(\phi^{1/2} p(s^2 \mid \phi) \right)
\end{align}
where the above expression includes the sum of two quadratic expressions in the exponential.   This almost looks like a normal. Can these be combined to form one quadratic expression that looks like a normal density?  Yes!   This is known as "completing the square".
Taking the a normal distribution for a parameter $\mu$ with mean $m$ and precision $p$, the quadratic term in the exponential may be expanded as
$$p(\mu - m)^2 = p\mu^2 - 2 p \mu m + p m^2$$
where we can read off that the precision is the term that multiplies the quadratic in $\mu$ and term that multiplies the linear term in $\mu$ is the product of  two times the mean and precision; if we know the precision, we can identify the mean.  The last term is the precision times the mean squared.
For our posterior,  we need to expand  the quadratics and recombine
terms to identify the new precision (the coefficient multiplying the quadratic in $\mu$) and the new mean  and completing the square or quadratic so that it may be factored.  Any left over terms will be independent of $\mu$ but may depend on $\phi$.  Applying to our case we have
\begin{align*}
- \frac 1 2  n \phi (\bar{D} - \mu)^2  - \frac 1 2  n_0 \phi (\mu - m_0)^2  & = 
-\frac 1 2 \left(\phi( n + n_0) \mu^2 - 2 \phi \mu (n \bar{D} + n_0 m_0) + \phi (n \bar{D}^2 + n_0 m_0^2) \right)  
\end{align*}
where we can read off that the posterior precision is $\phi(n + n_0)$.   The linear term is not yet of the form of the posterior precision times the posterior mean (times 2), but if we multiply and divide by $n + n_0$ it does satisfy that
 \begin{equation}-\frac 1 2 \left(\phi( n + n_0) \mu^2 - 2 \phi ( n + n_0) \mu \frac{(n \bar{D} + n_0 m_0) } {n + n_0} + \phi (n \bar{D}^2 + n_0 m_0^2) \right)  \label{eq:quad}
 \end{equation}
 so that we may identify that the posterior mean is $(n \bar{D} + n_0 m_0) /(n + n_0)$ which combined with the precision (or inverse variance) is enough to identity the conditional posterior distribution for $\mu$.  This leads to the result
$$ \mu \mid \phi, \data \sim \No(m_n, (\phi n_n)^{-1})
$$
where $m_m = (n \bar{D} + n_0 m_0) /(n + n_0)$ a weighted average of the sample mean and the prior mean, and
$n_n = n + n_0$ the sample and prior combined sample size.   This is exactly the result from earlier, but written in terms of relative prior precision and sampling precision $n_0$ and $n$ respectively to obtain the relative (to $\phi$) posterior precision $n_n = n + n_0$.






\subsection*{Conjugate prior for $\phi$}
Since $\sigma^2$ and $\phi$ can only take on values greater than zero and are continuous rather than discrete,  any reasonable prior distribution needs to incorporate those constraints.  Out of the distributions that we have encountered so far,
the gamma distribution fits the bill and is in fact the conjugate prior distribution for $\phi$.  We will use the following parameterization
$$
\phi \sim \Ga(\nu_0/2, \nu_0 s^2_0/2)
$$
with hyperparameters  $\nu_0$ (the prior degrees of freedom) and a rate parameter $\nu_0 s^2_0$ where $s^2_0$ is the best prior estimate of $\sigma^2$  (based on real or imaginary data) with prior degrees of freedom $\nu_0$ with a density
$$p(\phi) = \frac{1}{\Gamma(\nu_0/2)} (\nu_0 s^2_0 )^{\nu_0/2} \phi^{\nu_0/2 - 1} e^{- \phi \frac{\nu_0 s^2_0} {2}}
$$

\subsection*{Joint Posterior}
Together these form what is called a {\bf Normal-Gamma}($m_0, n_0, \nu_0, s^2_0$) family of distributions for $\mu, \phi$:
\begin{equation}
p(\mu, \phi) = \frac{(n_0 \phi)^{1/2}} {\sqrt{2\pi}} e^{- \frac{\phi n_0}{2} (\mu -m_0)^2} \frac{1}{\Gamma{\nu_0/2}} (\nu_0 s^2_0 )^{\nu_0/2 -1} e^{- \phi \frac{\nu_0 s^2_0} {2}}
\label{eq:NG}
\end{equation}
based on taking the product of the conditional normal distribution for $\mu$ given $\phi$ and the marginal Gamma  distribution for $\phi$.

Using the Normal-Gamma prior, the joint posterior is proportional to
$$
p(\mu, \phi \mid \data) \propto {\cal{L}}(\mu, \phi) p(\mu \mid \phi) p(\phi)
)
$$
the likelihood in (\ref{eq:lik}) times the prior in (\ref{eq:NG}).
is proportional to the product of the likelihood and priors.
If we substitute all of the above expressions for the likelihood and priors and simplify we can show that the posterior is in the Normal-Gamma family.  When we found the conditional normal posterior for $\mu$ given $\phi$, we stopped after identifying the mean and variance.  The extra terms that are needed to complete the square involve $\phi$ so for the joint posterior distribution, we need to be a bit more careful about keeping track of terms.  



\subsection*{Conjugate Posterior Distribution}
Given the data $\bar{D}$, $n$, $\nu$ and $s^2$ the Normal-Gamma prior is updated to obtain posterior distribution which is Normal-Gamma($m_n, n_n, \nu_n, s^2_n$) where the posterior hyperparameters are obtained using the following updating rules
\begin{itemize}
\item $m_n$: posterior mean of $\mu$  
$$m_n = \frac{n \bar{D} + n_0 m_0}{n + n_0}$$
which is a weighted combination of the sample mean and the prior mean with weights proportional to the relative precisions.
\item $n_n$:  relative posterior precision of the estimate $n_n = n + n_0$  based on combined observed sample size and prior sample size.
\item $\nu_n$: posterior degrees of freedom $\nu_n = \nu + \nu_0 + 1$ where the extra 1 comes from the distribution on $\mu$
\item $s^2_n$: posterior scale (squared)  
$$ s^2_n = \frac{ s^2 \nu + s^2_0 \nu_0 +  \frac{n n_0} {n + n_0} (\bar{D} - m_0)^2}{\nu_n} $$
which combines the observed sum of squared deviations of the data,  from the sample mean ($\nu s^2$), the prior sum of squares ($\nu_0 s^2_0$), and the last term which is deviation of the observed sample mean from the prior mean. If our   prior mean is v<ery far from the sample mean, this may in fact increase our posterior uncertainty, although this effect goes away as the sample size increases.
\end{itemize}



\subsection*{Marginal Distribution for $\mu$}
The conditional distribution for $\mu$ given $\phi$ is normal with mean $m_m$ and
variance $1/(n_n \phi)$, however, this does not directly help for obtaining credible intervals or inference as $\phi$ is unknown.
For posterior inference about $\mu$ we need to obtain the marginal distribution by
"averaging"  over the posterior uncertainty of $\phi$.  This requires integration (which we will show in the derivations), so for now we simply state the  result 
$$
\mu \mid \data  \sim \St_{\nu_n}(m_n, s^2_n/n_n)  \text{ or }  \frac{\mu - m_n}{\sqrt{(s^2_n/n_n)}} \sim \St_{\nu_n}(0,1)
$$
that $\mu$ given the data has a Student $t$ distribution centered at  $m_n$ (the posterior mean) and with a scale parameter that is $s^2_n/n_n$.   As with normals, subtracting the mean and dividing by the square root of the scale parameter leads to a scale-free distribution centered at 0.  Credible intervals or highest posterior density intervals with coverage $(1 - \alpha) 100\%$ may be obtained by taking 
$$m_n \pm t_{1 - \alpha/2, \nu_n} s_n$$



\subsection*{Reference Prior}

If you wish to use the Bayesian interpretation of probability, but want to try to be as objective as possible, you might think that a reasonable approach would be to construct your imaginary prior data letting your prior sample size and degrees of freedom go to zero.  A limiting case of the conjugate Normal-Gamma prior is what is refered to as a reference prior for $\mu, \phi$ and corresponds to taking $m_0 = n_0 = s^2_0 = 0$ but letting $\nu_0 = -1$.   
The negative prior degrees of freedom do not make any sense,  but this slight difference is important as the resulting  prior distribution does not depend on the units of measurenent, a form of "invariance".   While this is not a proper prior distribution, it does lead to 
a proper posterior distribution for $\mu$ 
$$
\mu \mid \data  \sim \St_{\nu}(\bar{D}, s^2/n)  \text{ or }  \frac{\mu - \bar{D}}{\sqrt{(s^2/n}}  \mid \data \sim \St_{\nu}(0,1)
$$
where $\nu = n - 1$.  (Technically it is proper if we have at least 2 observations).
The right hand distribution has the same form as the sampling distribution  for $\bar{D}$ (when conditioning on $\mu$), providing a duality between the frequentist and Bayesian paradigms for estimation, e.g.  95\% credible intervals ar of the form
$$\bar{D} \pm t_{1 - \alpha/2, n - 1} s/\sqrt{n}$$
where  $t_{1 - \alpha/2, n - 1}$  is the usual Student $t$
This allows the objective Bayesian to calculate a classical 
confidence interval, while providing the Bayesian probabilistic interpretation of the interval.

\section*{Bayes Factors and Hypothesis Testing}

The following were the hypotheses of interest in terms of the original parameters and the mean of the differences:
\begin{description}%[means are different]
\item [no differences] $H_1:  \mu_B = \mu_S  \Leftrightarrow \mu = 0 $   
\item [means are different] $H_2:  \mu_B \neq \mu_S \Leftrightarrow \mu  \neq 0 $ 
\item [sub-hypotheses]  $H_{3}:  \mu_B > \mu_S \Leftrightarrow \mu  > 0 $ 
\item [ ] $H_{4}:  \mu_B < \mu_S \Leftrightarrow \mu  < 0 $
\end{description}

It should be clear that $H_3$ and $H_4$ are included in $H_2$, so that we first need to find the probability of $H_1$ and $H_2$.    To find the posterior probabilities, we start with the Bayes factor for comparing $H_1$  to $H2$,
$$
\BF[H_1: H_2] = \frac{p(\data \mid H_1)} {p(\data \mid H_2)}
$$
which depends on the prior predictive distribution of the data or sufficient statistics $\bar{D}$ and $s^2$ under the two hypotheses. 

To start, lets look at the distribution of $\bar{D}$.
One way to think of $\bar{D}$ is that it is a noisy version of the population mean
$$\bar{D} = \mu + \epsilon$$
where $\epsilon$ has a normal distribution with mean 0 and variance $1/(\phi n)$.
Under $H_1$, 
since our prior distribution for $\mu$ was also normal and independent of the added noise (given $\phi$), we can add these two normal variables or sources of variation to get the prior predictive distribution of $\bar{D}$ (given $\phi$).  The sums of two independent normals are normal, with the mean obtained by adding the two means, $m_0 + 0$, and the variance is the sum of the two variances, $\frac{1}{\phi n} + \frac{1} {\phi n_0}$, so that
$$
\bar{D} \mid \phi \sim \No\left(m_0, \frac{1}{\phi} \left(\frac 1 n + \frac 1 {n_0}\right) \right)
$$
where the prior predictive variance combines the uncertainty due to sampling variation and our prior uncertainty.  Of course, we do not know $\phi$ so if we average over our prior uncertainty for $\phi$, the resulting predictive distribution would be a Student $t$ distribution
$$
\bar{D} \sim \St_{\nu_0}\left(m_0, s^2_0   \left(\frac 1 n + \frac 1 {n_0}\right)\right)
$$
with degrees of freedom $\nu_0$, location $m_0$ and squared scale parameter 
$s^2_0   \left(\frac 1 n + \frac 1 {n_0}\right)$, or the standardized version where
$$
\frac{\bar{D} - m_0} {s_0   \sqrt{\left(\frac 1 n + \frac 1 {n_0}\right)}} \sim \St_{\nu_0}(0,1)
$$
a standard $t$ distribution with $\nu_0$ degrees of freedom.  The extra uncertainty due to the unknown variance leads to a distribution with heavier tails and wider intervals.  The derivation to show that this has a $t$ distribution  follows along the same lines as showing that the posterior for $\mu$ was a student $t$  (see section at the end), but where we use the distribution for $\bar{D}$  rather than $\mu$ given $\phi$.

The above derivation does not include the data from $s^2$.  Let's see how we can incorporate that.
From Bayes theorem we have that conditional on $H_i$ (for $i$ equal 1 or 2) that
$$p(\mu, \phi  \mid \data, H_i) = \frac{p(\mu, \phi \mid H_i) p(\data \mid \mu, \phi, H_i)}{p(\data \mid H_i)}$$.
If we happen to know the conjugate updating rules and the forms of the densities then we can solve for $p(\data \mid H_i)$ as
$$
p(\data \mid H_i) = \frac{p(\mu, \phi  \mid H_i) p(\data \mid \mu, \phi, H_i)}{p(\mu, \phi \mid \data, H_i)}
$$
For those that are comfortable with integration,
$$
p(\data \mid H_i) = \int_0^\infty \int_{-\infty}^\infty p(\mu, \phi  \mid H_i) p(\data \mid \mu, \phi, H_i) d \mu \, d \phi.
$$
With some algebra we can simplify the expression of the ratio of the predictive distributions of the data to find the Bayes factor. 


Under a limiting case with $\nu_0 =s^2_0 = 0$ the Bayes factor is   
    $$
   \BF[H_1 : H_2] = \left(\frac{n + \n0}{\n0} \right)^{1/2} \left(
  \frac{ t^2  \frac{\n0}{n + \n0} + \nu }
  { t^2  + \nu} \right)^{\frac{\nu + 1}{2}}
    $$
which is a function of the 
\begin{itemize}
\item t-statistic $$t = \frac{|\bar{D}|}{s/\sqrt{n}}$$   
\item sample standard deviation $s$ 
\item degrees of freedom $\nu = n -1$
\end{itemize}

This provides a way to provide a posterior probability of the hypothesis through the Bayes factor that depends on the usual $t$ statistic.  Under equal prior odds the posterior probability of $H_1$ is 
$$
p(H1 \mid \data) = 1/( 1 + 1/\BF[H_1: H_2])
$$
\vspace{.5in}


\subsection*{R Code}

Let's define a function to help simplify the calculations of the posterior probabilities and the Bayes factor using the normal prior
$$
\mu \mid \sigma^2, H_2 \sim N(\mu_0, \sigma^2/n_0)
$$
while under $H_1$,  $\mu$ is exactly zero.  Under both hypotheses, we will use the "reference" prior
$$
p(\sigma^2) \propto 1/\sigma^2.
$$

\begin{Schunk}
\begin{Sinput}
> bayes.t.test = function(x, n0=1, mu0 = 0,  prior.H1=.5) {
+   out = t.test(x - mu0)
+   t = as.numeric(abs(out$statistic))
+   n = length(x)
+   df = n-1
+   # BF is BF of H1 to H2
+   BF=exp(.5*(log(n + n0) - log(n0) +
+                  (df + 1)*(log(t^2*n0/(n + n0) + df) -
+                            log(t^2 + df))))
+   PO= BF*prior.H1/(1 - prior.H1)
+   post.prob = 1/(1 + 1/PO)
+   return(list(BF.H1.H2=BF, post.prob.H1 = post.prob,
+               post.prob.H2= 1 - post.prob,
+               t=t, p.value=out$p.value, df=n-1))
+ }
\end{Sinput}
\end{Schunk}

Applying the function to the zinc data:
\begin{Schunk}
\begin{Sinput}
> out = bayes.t.test(zinc$difference)
> out
\end{Sinput}
\begin{Soutput}
$BF.H1.H2
[1] 0.01539321

$post.prob.H1
[1] 0.01515985

$post.prob.H2
[1] 0.9848402

$t
[1] 4.863813

$p.value
[1] 0.0008911155

$df
[1] 9
\end{Soutput}
\end{Schunk}
 we obtain BF, posterior probabilities, the t-statistics and p-value, where $H_1$ is that the mean difference $\mu$ is 0 while $H_2$ is that the mean difference is not zero, but {\it a priori} our uncertainty is described by the normal distribution. To obtain the Bayes factor for H2 to H1, we simply take one over the Bayes factor:
\begin{Schunk}
\begin{Sinput}
> 1/out$BF.H1.H2
\end{Sinput}
\begin{Soutput}
[1] 64.96373
\end{Soutput}
\end{Schunk}

The Bayes factor for comparing $H_1$ to $H_2$ is $0.0154$ which provides evidence against  $H_1$ or  equivalently the evidence in favor of $H_2$ is  $64.96$.  Under  equal prior odds, the 
posterior probability of $H_1$ is
$$
P(H_1 \mid \text{data}) = \frac{0.0154}{ 1 + 0.0154 } = 0.0152
$$
while  the p-value $\approx  0.00089$.  Both the p-value and posterior probability suggest evidence against $H_1$, however the p-value is obtained assuming that $H_1$ is true and finds the probability of data as extreme or more extreme than what we actually observed, while the posterior probability is comparing the likelihoods of the hypotheses through the predictive distribution of the data under each of the hypotheses which are evaluated at the observed data, and not data that we might have observed.


Note: the function that we defined could be used for any one sample hypothesis test of $\mu = \mu_0$ versus $\mu \neq \mu_0$.  

\subsection*{Derivation of Student $t$ distribution (optional)}

If $\mu$ given $\sigma^2$ (and the data) has a normal distribution with mean $m_m$ and variance $\sigma^2/n_n$ and $1/\sigma^2 \equiv \phi$ (given the data) has a Gamma distribution with shape parameter 
$\nu_n/2$ and rate parameter $\nu_n s^2_n/2$ 
\begin{align*}
\mu \mid \sigma^2, \data & \sim N(m_m, \sigma^2/n_n) \\ 
1/\sigma^2 \mid \data & \sim \Ga(\nu_n/2, \nu_n s^2_n/2) 
\intertext{then}
\mu \mid  \data & \sim t_{\nu_n}(m_m, s^2_n/n_n)
\end{align*}
a student $t$ distribution with mean $m_m$ and scale $s^2_n/n_n$ with degrees of freedom $\nu_n$.   This applies to the prior as well, so that without any data we use the prior hyper-parameters $m_0$, $n_0$, $\nu_0$ and $s^2_0$ in place of the updated values with the subscript $n$.  

To simplify notation, we'll substitute $\phi = 1/\sigma^2$.  The marginal distribution for $\mu$ is obtained by averaging over the values of $\sigma^2$.  Since $\sigma^2$ takes on continuous values rather than discrete, this averaging is represented as an integral
\begin{align*}
p(\mu \mid \data) & = \int_0^\infty p(\mu \mid \phi, \data) p(\phi \mid \data) d\phi \\
 & = \int_0^\infty 
\frac{1}{\sqrt{2 \pi}} (n_n \phi)^{1/2} 
e^{\left\{ - \frac{n_n \phi}{2} (\mu - m_n)^2 \right\}}
\frac{1}{\Gamma(\nu_n/2)} \left(\frac{\nu_n s^2_n}{2}\right)^{\nu_n/2}
\phi^{\nu_n/2 - 1} e^{\left\{- \phi \nu_n s^2_n/2\right\}} \, d\phi \\
 & =  
\left(\frac{n_n}{2 \pi}\right)^{1/2}\frac{1}{\Gamma\left(\frac{\nu_n}{2}\right)} \left(\frac{\nu_n s^2_n}{2}\right)^{\nu_n/2} \int_0^\infty  \phi^{(\nu_n +1)/2 - 1}
e^{\left\{ - \phi \left( \frac{n_n  (\mu - m_n)^2 + \nu_n s^2_n}{2} \right)\right\}} \, d\phi
\intertext{where the terms inside the integral are the "kernel" of a Gamma density.  We can multiply and divide by the normalizing constant of the Gamma density}
p(\mu \mid \data) & =  
\left(\frac{n_n}{2 \pi}\right)^{1/2}\frac{1}{\Gamma\left(\frac{\nu_n}{2}\right)} 
\left(\frac{\nu_n s^2_n}{2}\right)^{\nu_n/2} 
\Gamma\left(\frac{\nu_n + 1}{2}\right) 
\left( \frac{n_n  (\mu - m_n)^2 + \nu_n s^2_n}{2} \right)^{- \frac{\nu_n + 1}{2}}  \times\\
& \qquad \int_0^\infty  \frac{1}{\Gamma\left(\frac{\nu_n + 1}{2}\right)}
\left( \frac{n_n  (\mu - m_n)^2 + \nu_n s^2_n}{2} \right)^{ \frac{\nu_n + 1}{2}} \phi^{(\nu_n +1)/2 - 1}
e^{\left\{ - \phi \left( \frac{n_n  (\mu - m_n)^2 + \nu_n s^2_n}{2} \right)\right\}} \, d\phi
\intertext{so that the term in the integral now integrates to one and the resulting distribution is}
p(\mu \mid \data) & =  
\left(\frac{n_n}{2 \pi}\right)^{1/2}\frac{\Gamma\left(\frac{\nu_n + 1}{2}\right) }{\Gamma\left(\frac{\nu_n}{2}\right)} 
\left(\frac{\nu_n s^2_n}{2}\right)^{\nu_n/2} 
\left( \frac{n_n  (\mu - m_n)^2 + \nu_n s^2_n}{2} \right)^{- \frac{\nu_n + 1}{2}}.
\intertext{After some algebra this simplifies to}
p(\mu \mid \data) & =  
\frac{1}{\sqrt{\pi \nu_n s^2_n/n_n}}
\frac{\Gamma\left(\frac{\nu_n + 1}{2}\right) }
     {\Gamma\left(\frac{\nu_n}{2}\right)} 
\left( 1 +  \frac{1}{\nu_n}\frac{(\mu - m_n)^2}{s^2_n/n_n} \right)^{- \frac{\nu_n + 1}{2}}
\intertext{and is a more standard representation for a Student $t$ distribution and the  kernel of the density is the right most term.}
\end{align*}

\end{document}
