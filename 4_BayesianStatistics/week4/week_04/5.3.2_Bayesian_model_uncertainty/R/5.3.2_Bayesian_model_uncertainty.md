Bayesian model uncertainty
================
Dr Merlise Clyde

**Read In Data and Preprocess**

The data are available as a "dta" file from Gelman's website. You will need to load the `foreign` library to be able to read the file in as a dataframe.

``` r
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")

cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","iq", "work", "age") 
```

*Note: you do not need to use the as.numeric function to convert them to 0 or 1 values and could leave them as TRUE/FALSE, however, since the "levels"" appear in the labels in the plot I converted them so that the labels were shorter. Similarly, the variable names were shortened also for cosmetic reasons for the slides only.*

### Fit all possible BIC models

Use `BAS` package to fit all possible BIC models.

``` r
library(BAS)
cog_bas = bas.lm(kid_score ~ ., 
                 prior="BIC", 
                 modelprior=uniform(),
                 data=cognitive)
```

The `uniform` modelprior says that every variable is included with probabilty 1/16. By default, `BAS` will enumerate all possible models.

The object is of class `bas` for which there are `print`, `summary`, `plot`, `coef`, `fitted` and `predict` functions available.

The `bas` object has many slots for the output that you can extract and use.

``` r
names(cog_bas)
```

    ##  [1] "probne0"     "which"       "logmarg"     "postprobs"   "priorprobs" 
    ##  [6] "sampleprobs" "mse"         "mle"         "mle.se"      "shrinkage"  
    ## [11] "size"        "R2"          "namesx"      "n"           "prior"      
    ## [16] "modelprior"  "alpha"       "n.models"    "n.vars"      "Y"          
    ## [21] "X"           "mean.x"      "call"

### Summary of `BAS` models

`BAS` has an S3 method defined to provide a summary of the output from `bas.lm`.

``` r
summary(cog_bas)  # video uses round(summary(cog_bas),3)
```

    ##      Intercept hs iq work age         BF PostProbs     R2 dim   logmarg
    ## [1,]         1  1  1    0   0 1.00000000    0.5294 0.2141   3 -2583.135
    ## [2,]         1  0  1    0   0 0.56164897    0.2973 0.2010   2 -2583.712
    ## [3,]         1  0  1    1   0 0.10935932    0.0579 0.2061   3 -2585.349
    ## [4,]         1  1  1    1   0 0.08761320    0.0464 0.2163   4 -2585.570
    ## [5,]         1  1  1    0   1 0.06059028    0.0321 0.2150   4 -2585.939

The `summary` provides summaries of the top 5 models. Models are sorted in order of their posterior probabilities.

-   0's and 1's for the columns under variable names indicate if the variable is included in that model (row)
-   BF is the Bayes Factor of each model to the "best" model
-   PostProbs is the posterior probability of the model
-   R2 is the ordinary R2 in regression
-   dim is the number of predictors + 1 for the intercept
-   logmarg is the log of the marginal likelihood of the model
-   BIC is equal `-2*logmarg`

The top 2 models contain about 83 percent of the posterior mass over models.

The `print` method returns the call and the marginal posterior inclusions probabilites of the variables.

``` r
print(cog_bas)
```

    ## 
    ## Call:
    ## bas.lm(formula = kid_score ~ ., data = cognitive, prior = "BIC",     modelprior = uniform())
    ## 
    ## 
    ##  Marginal Posterior Inclusion Probabilities: 
    ## Intercept         hs         iq       work        age  
    ##   1.00000    0.61064    1.00000    0.11210    0.06898
