---
title: "Bayesian model uncertainty"
author: Dr Merlise Clyde
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Read In Data and Preprocess**

The data are available as a "dta" file from Gelman's website. You will need to load the `foreign` library to be able to read the file in as a dataframe. 


```{r data}
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")

cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","iq", "work", "age") 
```

*Note: you do not need to use the as.numeric function to convert them to 0  or 1 values and could leave them as TRUE/FALSE, however, since the "levels"" appear in the labels in the plot I  converted them so that the labels were shorter.  Similarly, the variable names were shortened also for cosmetic reasons for the slides only.*


###  Fit all possible BIC models ###

Use `BAS` package to fit all possible BIC models.

```{r}
library(BAS)
cog_bas = bas.lm(kid_score ~ ., 
                 prior="BIC", 
                 modelprior=uniform(),
                 data=cognitive)
       
```
The `uniform` modelprior says that every variable is included with probabilty 1/16.   By default, `BAS` will enumerate all possible models.

The object is of class `bas`  for which there are `print`, `summary`, `plot`, `coef`, `fitted` and `predict` functions available.

The `bas` object has many slots for the output that you can extract and use.
```{r names}

names(cog_bas)
```



### Summary of `BAS` models ###

`BAS` has an S3 method defined to provide a summary of the output from `bas.lm`.

```{r summary}
summary(cog_bas)  # video uses round(summary(cog_bas),3)
```

The `summary` provides summaries of the top 5 models.  Models are sorted in order of their posterior probabilities.

* 0's and 1's for the columns under variable names indicate if the variable is included in that model (row)
* BF is the Bayes Factor of each model to the "best" model
* PostProbs is the posterior probability of the model
* R2 is the ordinary R2 in regression
* dim is the number of predictors + 1 for the intercept
* logmarg is the log of the marginal likelihood of the model
* BIC is equal `-2*logmarg`

The top 2 models contain about 83 percent of the posterior mass over models.

The `print` method returns the call and the marginal posterior inclusions probabilites of the variables.
```{r print}

print(cog_bas)
```

