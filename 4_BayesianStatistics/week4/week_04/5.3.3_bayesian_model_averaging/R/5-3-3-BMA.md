5.3.3 Bayesian model averaging
================
Dr. Merlise Clyde, Duke University

**Read In Data and Preprocess**

The data are available as a "dta" file from Gelman's website. You will need to load the `foreign` library to be able to read the file in as a dataframe.

``` r
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","iq", "work", "age") 
```

*Note: you do not need to use the as.numeric function to convert them to 0 or 1 values and could leave them as TRUE/FALSE, however, since the "levels"" appear in the labels in the plot I converted them so that the labels were shorter. Similarly, the variable names were shortened also for cosmetic reasons for the slides only.*

**Bayesian Model Averaging using BAS**

You will need to install the `BAS` package from `CRAN` and load the library.

``` r
library(BAS)
```

We will use a "prior" that leads to log marginals corresponding to BIC and treat all models as being equally likely a priori.

``` r
cog_bas = bas.lm(kid_score ~ hs + iq + work + age,
                prior="BIC",
                modelprior=uniform(),
                data=cognitive)
cog_bas
```

    ## 
    ## Call:
    ## bas.lm(formula = kid_score ~ hs + iq + work + age, data = cognitive,     prior = "BIC", modelprior = uniform())
    ## 
    ## 
    ##  Marginal Posterior Inclusion Probabilities: 
    ## Intercept         hs         iq       work        age  
    ##   1.00000    0.61064    1.00000    0.11210    0.06898

``` r
cog_bas$probne0
```

    ## [1] 1.00000000 0.61063772 1.00000000 0.11210135 0.06897758

Printing the object provides the marginal posterior inclusion probabilities. The marginal posterior inclusion probability is the probability that predictor variable *j* is included in the model, *P*(*γ*<sub>*j*</sub> = 1 ∣ *Y*), where *γ*<sub>*j*</sub> is the indicator that predictor *j* is included in the model. This is equivalent to saying that the corresponding *β*<sub>*j*</sub> ≠ 0 or finding *P*(*β*<sub>*j*</sub> ≠ 0 ∣ *Y*). Under BMA, we can calculate this

*P*(*β*<sub>*j*</sub> ≠ 0 ∣ *Y*)=∑<sub>*m*</sub>*I*(*β*<sub>*j*</sub> ≠ 0 ∣ *M*<sub>*m*</sub>)*P*(*M*<sub>*m*</sub> ∣ *Y*)

or

*P*(*γ*<sub>*j*</sub> = 1 ∣ *Y*)=∑<sub>*m*</sub>*I*(*γ*<sub>*j*</sub> = 1 ∣ *M*<sub>*m*</sub>)*P*(*M*<sub>*m*</sub> ∣ *Y*)

which corresponds to adding up the posterior probabilities of all models where *β*<sub>*j*</sub> is not zero for the *j*th predictor. We can also extract this directly from the object as

``` r
cog_bas$probne0
```

    ## [1] 1.00000000 0.61063772 1.00000000 0.11210135 0.06897758

**Visualization of the Model Space**

To visualize the space of models (by default the top 20 models in terms of their posterior probabilities), we may use the `image` function.

``` r
image(cog_bas, rotate=F)
```

![](5-3-3-BMA_files/figure-markdown_github/unnamed-chunk-1-1.png)

**Coefficient Summaries**

To calculate the posterior distributions of the coefficients, we use the `coef` function create an object with posterior means and standard deviations.

``` r
cog_coef = coef(cog_bas)
cog_coef
```

    ## 
    ##  Marginal Posterior Summaries of Coefficients: 
    ##            post mean  post SD   post p(B != 0)
    ## Intercept  86.79724    0.87287   1.00000      
    ## hs          3.59494    3.35643   0.61064      
    ## iq          0.58101    0.06363   1.00000      
    ## work        0.36696    1.30939   0.11210      
    ## age         0.02089    0.11738   0.06898

Typing the name of the `coef` object returns the marginal posterior mean, standard deviation and posterior inclusion probabilities obtained by BMA.

**Posterior Densities under BMA**

To plot the posterior distributions of the four regression coefficents, use the `plot` function.

``` r
par(mfrow=c(2,2))
plot(cog_coef, subset=c(2:5), ask=F)
```

![](5-3-3-BMA_files/figure-markdown_github/unnamed-chunk-2-1.png)

The optional subset argument lets you select which coefficients to plot from the indices `1:(p+1)`. In this case the intercept `subset=1` has been omitted to create the two by two array of plots.

To obtain the numerical summaries of means, standard deviations and posterior inclusion probabilites, simply type the name of the coefficient object.

``` r
cog_coef
```

    ## 
    ##  Marginal Posterior Summaries of Coefficients: 
    ##            post mean  post SD   post p(B != 0)
    ## Intercept  86.79724    0.87287   1.00000      
    ## hs          3.59494    3.35643   0.61064      
    ## iq          0.58101    0.06363   1.00000      
    ## work        0.36696    1.30939   0.11210      
    ## age         0.02089    0.11738   0.06898

We can also extract credible intervals using the `confint` function.

``` r
confint(cog_coef, nsim=10000)
```

    ##                  2.5  %    97.5  %        beta
    ## Intercept  8.512633e+01 88.4675413 86.79723502
    ## hs         0.000000e+00  9.0376287  3.59493634
    ## iq         4.618659e-01  0.7049504  0.58100634
    ## work       0.000000e+00  3.9097505  0.36695715
    ## age       -5.621743e-05  0.2831693  0.02089334
    ## attr(,"Probability")
    ## [1] 0.95
    ## attr(,"class")
    ## [1] "confint.bas"

*Note: `BAS` currently centers all covariates. This does not change the slope coefficients, however the intercept in all models is $\\bar{Y}$. *

### Posterior inclusion probabilities

Finally calculate the probabiliy that the coefficient for age is zero. The marginal posterior inclusion probabilities are stored in the `bas` object as `probne0` or as mentioned above shown via printing the bas object.

``` r
# probability coefficients are zero
cog_bas$probne0
```

    ## [1] 1.00000000 0.61063772 1.00000000 0.11210135 0.06897758

``` r
cog_coef$probne0
```

    ## [1] 1.00000000 0.61063772 1.00000000 0.11210135 0.06897758

``` r
p = 4
p.age = round(1 - cog_coef$probne0[p+1], 3)

p.age  # probability coefficient for age is zero
```

    ## [1] 0.931

*The intercept is always in position 1, so we need to add one to p to extract the last coefficient corresponding to the variable age.*
