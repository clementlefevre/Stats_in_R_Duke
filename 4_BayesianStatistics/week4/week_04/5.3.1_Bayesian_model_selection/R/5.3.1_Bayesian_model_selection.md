Bayesian model selection
================
Dr Merlise Clyde

**Read In Data and Preprocess**

The data are available as a "dta" file from Gelman's website. You will need to load the `foreign` library to be able to read the file in as a dataframe.

``` r
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")

cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","iq", "work", "age") 
```

*Note: you do not need to use the as.numeric function to convert them to 0 or 1 values and could leave them as TRUE/FALSE, however, since the "levels"" appear in the labels in the plot I converted them so that the labels were shorter. Similarly, the variable names were shortened also for cosmetic reasons for the slides only.*

### Stepwise selection using BIC

``` r
n = nrow(cognitive)
cog.lm = lm(kid_score ~ ., data=cognitive)
cog.step = step(cog.lm, k=log(n))   # penalty for BIC rather than AIC
```

    ## Start:  AIC=2541.07
    ## kid_score ~ hs + iq + work + age
    ## 
    ##        Df Sum of Sq    RSS    AIC
    ## - age   1     143.0 141365 2535.4
    ## - work  1     383.5 141605 2536.2
    ## - hs    1    1595.1 142817 2539.9
    ## <none>              141222 2541.1
    ## - iq    1   28219.9 169441 2614.1
    ## 
    ## Step:  AIC=2535.44
    ## kid_score ~ hs + iq + work
    ## 
    ##        Df Sum of Sq    RSS    AIC
    ## - work  1     392.5 141757 2530.6
    ## - hs    1    1845.7 143210 2535.0
    ## <none>              141365 2535.4
    ## - iq    1   28381.9 169747 2608.8
    ## 
    ## Step:  AIC=2530.57
    ## kid_score ~ hs + iq
    ## 
    ##        Df Sum of Sq    RSS    AIC
    ## <none>              141757 2530.6
    ## - hs    1    2380.2 144137 2531.7
    ## - iq    1   28504.1 170261 2604.0

While the output is labeled as "AIC", the values are BIC. The best BIC model is at the end and includes `hs` and `iq`.

### Bayesian Models

We will use the `BAS` package to fit all possible BIC models.

``` r
library(BAS)
cog.bic = bas.lm(kid_score ~ ., data=cognitive, 
                 prior="BIC", 
                 modelprior=uniform())
```

The `uniform` modelprior says that every variable is included with probabilty 1/16.

### Find the best BIC model with `BAS`

``` r
best = which.max(cog.bic$logmarg)  # this is the best BIC model
bestmodel =  cog.bic$which[[best]]
bestmodel  # indices of the best model where 0 is the intercept
```

    ## [1] 0 1 2

``` r
bestgamma = rep(0, cog.bic$n.vars)
# convert to a binary  vector where 1 means the variable was selected
bestgamma[bestmodel + 1] = 1  
bestgamma  # use this to fit the best BIC model
```

    ## [1] 1 1 1 0 0

``` r
cog.bestbic = bas.lm(kid_score ~ ., data=cognitive,
                     prior="BIC", n.models=1,
                     bestmodel=bestgamma, 
                     modelprior= uniform())
```

### Posterior means, standard deviations, and credible intervals

To extract the posterior means and standard deviations we use the `coef` function.

``` r
cog.coef = coef(cog.bestbic)

out = confint(cog.coef)
names = c("post mean", "post sd", colnames(out))
coef.bic = cbind(cog.coef$postmean, cog.coef$postsd, out)
colnames(coef.bic) = names
coef.bic
```

    ##           post mean    post sd     2.5  %    97.5  %
    ## Intercept 86.797235 0.87054033 85.0862025 88.5082675
    ## hs         5.950117 2.21181218  1.6028370 10.2973969
    ## iq         0.563906 0.06057408  0.4448487  0.6829634
    ## work       0.000000 0.00000000  0.0000000  0.0000000
    ## age        0.000000 0.00000000  0.0000000  0.0000000

In `BAS` all variables have been centered so that the intercept is \(\bar{Y}\).

To use the `lm` code we will obtain

``` r
best.bic =  lm(kid_score ~ hs + iq, data=cognitive )
out = summary(best.bic)$coef[, 1:2]
colnames(out) = c("posterior mean", "sd")
out = cbind(out, confint(best.bic))
out
```

    ##             posterior mean         sd      2.5 %     97.5 %
    ## (Intercept)      25.731538 5.87520802 14.1839148 37.2791615
    ## hs                5.950117 2.21181218  1.6028370 10.2973969
    ## iq                0.563906 0.06057408  0.4448487  0.6829634

The posterior estimates and intervals for all coefficients except the intercept are the same using either `lm` or `BAS` and have the same Bayesian interpretation whether or not the predictor variables have been centered. Centering leads to the following linear predictor where we have added and subtracted the coefficients times the means of each predictor.

\[ \beta_0 + beta_1(hs - \bar{hs}) + beta_1 \bar{hs} + beta_2(iq - \bar{iq}) + beta_2 \bar{iq}\]

Combining the constant terms

\[ \beta_0 + beta_1 \bar{hs} + beta_2 \bar{iq} + beta_1(hs - \bar{hs}) + beta_2(iq - \bar{iq}) \]

and letting the new intercept be

\[ alpha \equiv \beta_0 + beta_1 \bar{hs} + beta_2 \bar{iq}\]

the mean function wiht the centered predictors is

\[ \alpha + beta_1(hs - \bar{hs}) + beta_2(iq - \bar{iq}) \]

Since this is a linear transformation we can easily transform the estimates and distributions.

Note that in the centered parameterization, the intercept is the mean of \(Y\) when all of the covariates are equal to their mean. As this does not require extrapolation, this is more meaningful than the usual parameterization for interpretation.
