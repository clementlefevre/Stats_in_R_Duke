---
title: "Bayesian model selection"
author: Dr Merlise Clyde
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Read In Data and Preprocess**

The data are available as a "dta" file from Gelman's website. You will need to load the `foreign` library to be able to read the file in as a dataframe. 


```{r data}
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")

cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =  as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","iq", "work", "age") 
```

*Note: you do not need to use the as.numeric function to convert them to 0  or 1 values and could leave them as TRUE/FALSE, however, since the "levels"" appear in the labels in the plot I  converted them so that the labels were shorter.  Similarly, the variable names were shortened also for cosmetic reasons for the slides only.*

### Stepwise selection using BIC ###
```{r stepwise}
n = nrow(cognitive)
cog.lm = lm(kid_score ~ ., data=cognitive)
cog.step = step(cog.lm, k=log(n))   # penalty for BIC rather than AIC
```

While the output is labeled as "AIC", the values are BIC.   The best BIC model is at the end and includes `hs` and `iq`.


###  Bayesian Models  ###

We will use the `BAS` package to fit all possible BIC models.

```{r}
library(BAS)
cog.bic = bas.lm(kid_score ~ ., data=cognitive, 
                 prior="BIC", 
                 modelprior=uniform())
       
```
The `uniform` modelprior says that every variable is included with probabilty 1/16. 


### Find the best BIC model with `BAS` ###


```{r fit best BIC model}
best = which.max(cog.bic$logmarg)  # this is the best BIC model
bestmodel =  cog.bic$which[[best]]
bestmodel  # indices of the best model where 0 is the intercept
bestgamma = rep(0, cog.bic$n.vars)
# convert to a binary  vector where 1 means the variable was selected
bestgamma[bestmodel + 1] = 1  
bestgamma  # use this to fit the best BIC model
cog.bestbic = bas.lm(kid_score ~ ., data=cognitive,
                     prior="BIC", n.models=1,
                     bestmodel=bestgamma, 
                     modelprior= uniform())


```

### Posterior means, standard deviations, and credible intervals ###

To extract the posterior means and standard deviations we use the `coef` function.

```{r}
cog.coef = coef(cog.bestbic)

out = confint(cog.coef)
names = c("post mean", "post sd", colnames(out))
coef.bic = cbind(cog.coef$postmean, cog.coef$postsd, out)
colnames(coef.bic) = names
coef.bic
```
In `BAS` all variables have been centered so that the intercept is $\bar{Y}$. 

To use the `lm` code we will obtain

```{r}
best.bic =  lm(kid_score ~ hs + iq, data=cognitive )
out = summary(best.bic)$coef[, 1:2]
colnames(out) = c("posterior mean", "sd")
out = cbind(out, confint(best.bic))
out
```

The posterior estimates and intervals for all coefficients except the intercept are the same using either `lm` or `BAS` and have the same Bayesian interpretation whether or not the predictor variables have been centered.  Centering leads to the following linear predictor where we have added and subtracted the coefficients times the means of each predictor.

$$ \beta_0 + beta_1(hs - \bar{hs}) + beta_1 \bar{hs} + beta_2(iq - \bar{iq}) + beta_2 \bar{iq}$$

Combining the constant terms

$$ \beta_0 + beta_1 \bar{hs} + beta_2 \bar{iq} + beta_1(hs - \bar{hs}) + beta_2(iq - \bar{iq}) $$

and letting the new intercept be

$$ alpha \equiv \beta_0 + beta_1 \bar{hs} + beta_2 \bar{iq}$$

the mean function wiht the centered predictors is 

$$ \alpha + beta_1(hs - \bar{hs}) + beta_2(iq - \bar{iq}) $$

Since this is a linear transformation we can easily transform the estimates and distributions.

Note that in the centered parameterization,  the intercept is the mean of $Y$ when all of the covariates are equal to their mean.  As this does not require extrapolation, this is more meaningful than the usual parameterization for interpretation.

